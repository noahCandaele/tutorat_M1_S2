{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire tous les imports nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants :  32988\n"
     ]
    }
   ],
   "source": [
    "# les components\n",
    "test_components_data = pd.read_csv('./data/test_components.csv')\n",
    "train_components_data = pd.read_csv('./data/train_components.csv')\n",
    "validation_components_data = pd.read_csv('./data/validation_components.csv')\n",
    "\n",
    "# ajouts des 2 colonnes de contexte aux components\n",
    "test_components_data['context1'] = ''\n",
    "test_components_data['context2'] = ''\n",
    "train_components_data['context1'] = ''\n",
    "train_components_data['context2'] = ''\n",
    "validation_components_data['context1'] = ''\n",
    "validation_components_data['context2'] = ''\n",
    "\n",
    "# les speechs\n",
    "test_speeches_data = pd.read_csv('./data/test_speeches.csv')\n",
    "train_speeches_data = pd.read_csv('./data/train_speeches.csv')\n",
    "validation_speeches_data = pd.read_csv('./data/validation_speeches.csv')\n",
    "\n",
    "# données de statistiques\n",
    "size_components_data = len(test_components_data) + len(train_components_data) + len(validation_components_data)\n",
    "print(\"Nombre de composants : \", size_components_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petite fonction pour les resultats intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction permettant de retourner string permettant un print en couleur sur un pourcentage en python\n",
    "# plus le pourcentage est proche de 0 plus il sera rouge, plus il est proche de 100 plus il sera vert\n",
    "# le pourcentage sera calculé en fonction de la taille du dataset et d'un chiffre donné\n",
    "def printResultColor(size_dataset, number):\n",
    "    pourcentage = number / size_dataset * 100\n",
    "    if pourcentage < 1:\n",
    "        # print vert\n",
    "        return(\"\\033[92m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    elif pourcentage < 5:\n",
    "        # print orange\n",
    "        return(\"\\033[93m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    else:\n",
    "        # print rouge\n",
    "        return(\"\\033[91m {0:0.3f}%\\033[0m\".format(pourcentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la premiere fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous components dans les Speeches. Une fois chaque Text trouvé, il faut completer le context 1 et 2.\n",
    "\n",
    "Cette fonction renvoie une liste de tous les identifiants des components qui n'ont pas été trouvés et une liste de tous les composants qui ont été trouvés plusieur fois (multiple speeches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find1Component(dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    multiple_speech = []\n",
    "    for index, row in dataset_component.iterrows():\n",
    "        textToFind = row.Text\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.find(textToFind) != -1]\n",
    "        # verifier que le component est bien dans un speech\n",
    "        if len(speeches) == 0:\n",
    "            not_found.append(index)\n",
    "        elif len(speeches) == 1:\n",
    "            # trouver le contexte 1 et 2\n",
    "            # tokenizer le speech à l'aide de nltk\n",
    "            sentences = sent_tokenize(speeches.iloc[0].Speech)\n",
    "            # trouver la position de la phrase contenant le component\n",
    "            position = -1\n",
    "            for i in range(len(sentences)):\n",
    "                if sentences[i].find(textToFind) != -1:\n",
    "                    position = i\n",
    "                    break\n",
    "            if position == -1:\n",
    "                # peut arriver si le component est en plusieurs phrases et donc la tokenizer ne le trouve pas\n",
    "                not_found.append(index)\n",
    "            else:\n",
    "                # trouver le contexte 1\n",
    "                if position > 0:\n",
    "                    dataset_component.at[index, 'context1'] = sentences[position - 1]\n",
    "                # ajouter la phrase contenant le component au contexte 1\n",
    "                dataset_component.at[index, 'context1'] += sentences[position]\n",
    "                # ajouter la phrase qui suit le component au contexte 1\n",
    "                if position < len(sentences) - 1:\n",
    "                    dataset_component.at[index, 'context1'] += sentences[position + 1]\n",
    "                \n",
    "                # mettre tout le speech dans le contexte 2\n",
    "                dataset_component.at[index, 'context2'] = speeches.iloc[0].Speech\n",
    "        else:\n",
    "            # plusieurs speeches contiennent le component\n",
    "            # il va falloir verifier l'id du speech et de la section pour verifier si c'est le bon\n",
    "            multiple_speech.append(index)\n",
    "    return not_found, multiple_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants test non trouvés: 102 -> \u001b[93m 1.074%\u001b[0m\n",
      "Nombre de composants test trouvés dans plusieurs discours: 1030 -> \u001b[91m 10.850%\u001b[0m\n",
      "\n",
      "Nombre de composants train non trouvés: 306 -> \u001b[93m 1.793%\u001b[0m\n",
      "Nombre de composants train trouvés dans plusieurs discours: 777 -> \u001b[93m 4.552%\u001b[0m\n",
      "\n",
      "Nombre de composants validation non trouvés: 109 -> \u001b[93m 1.696%\u001b[0m\n",
      "Nombre de composants validation trouvés dans plusieurs discours: 144 -> \u001b[93m 2.241%\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tester la fonction\n",
    "not_founded_test = []\n",
    "multiple_founded_test = []\n",
    "not_founded_test, multiple_founded_test = find1Component(test_components_data, test_speeches_data)\n",
    "print(\"Nombre de composants test non trouvés: {} -> {}\".format(len(not_founded_test), printResultColor(len(test_components_data), len(not_founded_test))))\n",
    "print(\"Nombre de composants test trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_test), printResultColor(len(test_components_data), len(multiple_founded_test))))\n",
    "\n",
    "not_founded_train = []\n",
    "multiple_founded_train = []\n",
    "not_founded_train, multiple_founded_train = find1Component(train_components_data, train_speeches_data)\n",
    "print(\"Nombre de composants train non trouvés: {} -> {}\".format(len(not_founded_train), printResultColor(len(train_components_data), len(not_founded_train))))\n",
    "print(\"Nombre de composants train trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_train), printResultColor(len(train_components_data), len(multiple_founded_train))))\n",
    "\n",
    "not_founded_validation = []\n",
    "multiple_founded_validation = []\n",
    "not_founded_validation, multiple_founded_validation = find1Component(validation_components_data, validation_speeches_data)\n",
    "print(\"Nombre de composants validation non trouvés: {} -> {}\".format(len(not_founded_validation), printResultColor(len(validation_components_data), len(not_founded_validation))))\n",
    "print(\"Nombre de composants validation trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_validation), printResultColor(len(validation_components_data), len(multiple_founded_validation))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la deuxième fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous les components qui sont en plusieurs phrases et qui n'ont pas été trouvés précédements, avec ou sans la ponctuation. Le resultat renvoyé sera similaire à celui de la fonction précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find2Components(idsComponents, dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    # pour chaque id de component\n",
    "    for idComponent in idsComponents:\n",
    "        # get le text du component\n",
    "        textToFind = re.sub(r'[\\.!\\?]?( -)?', '', dataset_component.iloc[idComponent].Text)\n",
    "        # find the text in the speeches, but this time we remove the ponctuations of the speeches using replace function of pandas\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.replace(r'[\\.!\\?]?( -)?', '', regex=True).str.find(textToFind) != -1]\n",
    "        if(len(speeches) == 1):\n",
    "            if(idComponent == 187):\n",
    "                print(\"ici\")\n",
    "            speeches = speeches.iloc[0]\n",
    "            # si on a trouvé le component dans un seul speech\n",
    "            # tokenizer le component en faisant un split sur les espaces\n",
    "            words_component = dataset_component.iloc[idComponent].Text.split(\" \")\n",
    "            full_component = \"\"\n",
    "            for word in words_component:\n",
    "                if(speeches.Speech.find(full_component) != -1):\n",
    "                    # si le component est dans le texte\n",
    "                    # on ajoute le mot suivant au component\n",
    "                    if(len(full_component) > 0):\n",
    "                        full_component += \" \"+word\n",
    "                    else:\n",
    "                        full_component = word\n",
    "                else:\n",
    "                    # il faut enlever le dernier mot du component\n",
    "                    # car il manque surement une ponctuation\n",
    "                    last = full_component[full_component.rfind(\" \"):]\n",
    "                    # remove the last word from the component\n",
    "                    full_component = full_component[:full_component.rfind(\" \")]\n",
    "                    letter = speeches.Speech[speeches.Speech.find(full_component)+len(full_component)]\n",
    "                    if(speeches.Speech.find(full_component+letter+last) != -1):\n",
    "                        full_component += letter+last+\" \"+word\n",
    "                    elif(speeches.Speech.find(full_component+\" \"+letter+last) != -1):\n",
    "                        full_component += \" \"+letter+last+\" \"+word\n",
    "        else:\n",
    "            not_found.append(idComponent)\n",
    "    print(\"Nombre de composants trouvés {} sur {}\".format(len(idsComponents) - len(not_found), len(idsComponents)))\n",
    "    return not_found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index 187 de component train:\n",
    "\n",
    "<font color=\"orange\"> I don't think it was helpful when he suggested that seventeen million people go to bed hungry every night in the United States</font>\n",
    "\n",
    "Le component du dataset est incomplet, voici le text original (id: 22 - speech 2 section 11 by Nixon):\n",
    "\n",
    "I did. And as I pointed out in 1952, (...) I don't think he should say that our prestige is at an all-time low. I think this is very harmful at a time Mr. Khrushchev is here - harmful because it's wrong. <font color=\"orange\">I don't think it was helpful when he suggested</font><font color=\"red\"> - and I'm glad he's corrected this to an extent - </font><font color=\"orange\">that seventeen million people go to bed hungry every night in the United States</font>. Now this just wasn't true. Now, there are people who go to bed hungry in the United States - far less, incidentally, than used to go to bed hungry when we came into power at the end of the Truman Administration. But the thing that is right about the United States, it should be emphasized, (...)\n",
    "\n",
    "\n",
    "Une partie du texte est manquant dans le speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants trouvés 201 sur 306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[187,\n",
       " 188,\n",
       " 189,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 1196,\n",
       " 1715,\n",
       " 2060,\n",
       " 2141,\n",
       " 2145,\n",
       " 2146,\n",
       " 2210,\n",
       " 2213,\n",
       " 2246,\n",
       " 2302,\n",
       " 2315,\n",
       " 2407,\n",
       " 2469,\n",
       " 2509,\n",
       " 2546,\n",
       " 2578,\n",
       " 2791,\n",
       " 2797,\n",
       " 2940,\n",
       " 3129,\n",
       " 3142,\n",
       " 3149,\n",
       " 3158,\n",
       " 3445,\n",
       " 3477,\n",
       " 3822,\n",
       " 3878,\n",
       " 3986,\n",
       " 4150,\n",
       " 4446,\n",
       " 4703,\n",
       " 5026,\n",
       " 5036,\n",
       " 5040,\n",
       " 5325,\n",
       " 5376,\n",
       " 5377,\n",
       " 5600,\n",
       " 5606,\n",
       " 5632,\n",
       " 5755,\n",
       " 5809,\n",
       " 5880,\n",
       " 5883,\n",
       " 5917,\n",
       " 5984,\n",
       " 5985,\n",
       " 6051,\n",
       " 6091,\n",
       " 6101,\n",
       " 8325,\n",
       " 9264,\n",
       " 9533,\n",
       " 9824,\n",
       " 9905,\n",
       " 10109,\n",
       " 10119,\n",
       " 10835,\n",
       " 10838,\n",
       " 10993,\n",
       " 11085,\n",
       " 13002,\n",
       " 13088,\n",
       " 13292,\n",
       " 13699,\n",
       " 13761,\n",
       " 13886,\n",
       " 14027,\n",
       " 14111,\n",
       " 14181,\n",
       " 14189,\n",
       " 14283,\n",
       " 14285,\n",
       " 14341,\n",
       " 14371,\n",
       " 14459,\n",
       " 14511,\n",
       " 14555,\n",
       " 14634,\n",
       " 14821,\n",
       " 14823,\n",
       " 14836,\n",
       " 14837,\n",
       " 14896,\n",
       " 14920,\n",
       " 14934,\n",
       " 14940,\n",
       " 14944,\n",
       " 14969,\n",
       " 14971,\n",
       " 14972,\n",
       " 14973,\n",
       " 14974,\n",
       " 15578,\n",
       " 16755,\n",
       " 16962,\n",
       " 16965]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find2Components(not_founded_train, train_components_data, train_speeches_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component trouvé: une phrase, avec des virgules aussi\n",
      "test Je suis une phrase, avec des virgules aussi ?! Je suis une autre phrase ? Je suis la dernière phrase !\n"
     ]
    }
   ],
   "source": [
    "# test avec 3 phrases\n",
    "text = \"Je suis une phrase, avec des virgules aussi ?! Je suis une autre phrase ? Je suis la dernière phrase !\"\n",
    "# exemple de component en plusieurs phrases sans ponctuation\n",
    "component_test = \"une phrase, avec des virgules aussi Je suis une autre\"\n",
    "\n",
    "# tokenize en splitant sur les espaces\n",
    "component_test_words = component_test.split(\" \")\n",
    "full_component = \"\"\n",
    "# pour chaque mot du component\n",
    "for word in component_test_words:\n",
    "    if(text.find(full_component) != -1):\n",
    "        # si le component est dans le texte\n",
    "        # on ajoute le mot suivant au component\n",
    "        if(len(full_component) > 0):\n",
    "            full_component += \" \"+word\n",
    "        else:\n",
    "            full_component = word\n",
    "    else:\n",
    "        # il faut enlever le dernier mot du component\n",
    "        # car il manque surement une ponctuation\n",
    "        last = full_component[full_component.rfind(\" \"):]\n",
    "        # remove the last word from the component\n",
    "        full_component = full_component[:full_component.rfind(\" \")]\n",
    "        letter = text[text.find(full_component)+len(full_component)]\n",
    "        if(text.find(full_component+letter+last) != -1):\n",
    "            full_component += letter+last+\" \"+word\n",
    "print(\"Component trouvé: {}\".format(full_component))\n",
    "\n",
    "print(\"test \"+text.replace(r'[\\.!\\?]', ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
