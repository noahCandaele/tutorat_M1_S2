{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire tous les imports nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants :  32988\n"
     ]
    }
   ],
   "source": [
    "# les components\n",
    "test_components_data = pd.read_csv('./data/test_components.csv')\n",
    "train_components_data = pd.read_csv('./data/train_components.csv')\n",
    "validation_components_data = pd.read_csv('./data/validation_components.csv')\n",
    "\n",
    "# ajouts des 2 colonnes de contexte aux components\n",
    "test_components_data['context1'] = ''\n",
    "test_components_data['context2'] = ''\n",
    "train_components_data['context1'] = ''\n",
    "train_components_data['context2'] = ''\n",
    "validation_components_data['context1'] = ''\n",
    "validation_components_data['context2'] = ''\n",
    "\n",
    "# les speechs\n",
    "test_speeches_data = pd.read_csv('./data/test_speeches.csv')\n",
    "train_speeches_data = pd.read_csv('./data/train_speeches.csv')\n",
    "validation_speeches_data = pd.read_csv('./data/validation_speeches.csv')\n",
    "\n",
    "# données de statistiques\n",
    "size_components_data = len(test_components_data) + len(train_components_data) + len(validation_components_data)\n",
    "print(\"Nombre de composants : \", size_components_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petite fonction pour les resultats intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction permettant de retourner string permettant un print en couleur sur un pourcentage en python\n",
    "# plus le pourcentage est proche de 0 plus il sera rouge, plus il est proche de 100 plus il sera vert\n",
    "# le pourcentage sera calculé en fonction de la taille du dataset et d'un chiffre donné\n",
    "def printResultColor(size_dataset, number):\n",
    "    pourcentage = number / size_dataset * 100\n",
    "    if pourcentage < 1:\n",
    "        # print vert\n",
    "        return(\"\\033[92m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    elif pourcentage < 5:\n",
    "        # print orange\n",
    "        return(\"\\033[93m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    else:\n",
    "        # print rouge\n",
    "        return(\"\\033[91m {0:0.3f}%\\033[0m\".format(pourcentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la premiere fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous components dans les Speeches. Une fois chaque Text trouvé, il faut completer le context 1 et 2.\n",
    "\n",
    "Cette fonction renvoie une liste de tous les identifiants des components qui n'ont pas été trouvés et une liste de tous les composants qui ont été trouvés plusieur fois (multiple speeches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find1Component(dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    multiple_speech = []\n",
    "    for index, row in dataset_component.iterrows():\n",
    "        textToFind = row.Text\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.find(textToFind) != -1]\n",
    "        # verifier que le component est bien dans un speech\n",
    "        if len(speeches) == 0:\n",
    "            not_found.append(index)\n",
    "        elif len(speeches) == 1:\n",
    "            # trouver le contexte 1 et 2\n",
    "            # tokenizer le speech à l'aide de nltk\n",
    "            sentences = sent_tokenize(speeches.iloc[0].Speech)\n",
    "            # trouver la position de la phrase contenant le component\n",
    "            position = -1\n",
    "            for i in range(len(sentences)):\n",
    "                if sentences[i].find(textToFind.strip()) != -1:\n",
    "                    position = i\n",
    "                    break\n",
    "            if position == -1:\n",
    "                # le component n'a pas été trouvé dans le speech car il est en plusieurs phrases\n",
    "                # ici nous allons donc faire un autre traitement pour trouver le contexte 1 et 2\n",
    "                # dans un premier temps nous allons tokenizer le component en phrases\n",
    "                component_sentences = sent_tokenize(textToFind)\n",
    "                # ensuite nous allons chercher la position de la phrase contenant le component\n",
    "                position = -1\n",
    "                positions = np.where([sentences[i].find(component_sentences[0].strip()) != -1 for i in range(len(sentences))])\n",
    "                for i in range(len(positions[0])):\n",
    "                    if sentences[positions[0][i]+1].find(component_sentences[1].strip()) != -1:\n",
    "                        position = positions[0][i]\n",
    "                        break\n",
    "                try:\n",
    "                    if(position != -1):\n",
    "                        # verifier que la suite du component est bien dans la phrase suivante\n",
    "                        if sentences[position + 1].find(component_sentences[1].strip()) != -1:\n",
    "                            # trouver le contexte 1\n",
    "                            if position > 0:\n",
    "                                dataset_component.at[index, 'context1'] = sentences[position - 1]\n",
    "                            # ajouter la phrase contenant le component au contexte 1\n",
    "                            dataset_component.at[index, 'context1'] += sentences[position]\n",
    "                            dataset_component.at[index, 'context1'] += sentences[position + 1]\n",
    "                            if position < len(sentences) - 2:\n",
    "                                # ajouter la phrase qui suit le component au contexte 1\n",
    "                                dataset_component.at[index, 'context1'] += sentences[position + 2]\n",
    "                            \n",
    "                            # mettre tout le speech dans le contexte 2\n",
    "                            dataset_component.at[index, 'context2'] = speeches.iloc[0].Speech\n",
    "                        else:\n",
    "                            print(\"La suite du component {} n'a pas été trouvés ?\".format(index))\n",
    "                    else:\n",
    "                        print(\"Le component {} n'a pas été trouvé en cherchant sur les phrases\".format(index))\n",
    "                except:\n",
    "                    print(\"Something went wrong with the component: {}\".format(textToFind))\n",
    "            else:\n",
    "                # trouver le contexte 1\n",
    "                if position > 0:\n",
    "                    dataset_component.at[index, 'context1'] = sentences[position - 1]\n",
    "                # ajouter la phrase contenant le component au contexte 1\n",
    "                dataset_component.at[index, 'context1'] += sentences[position]\n",
    "                # ajouter la phrase qui suit le component au contexte 1\n",
    "                if position < len(sentences) - 1:\n",
    "                    dataset_component.at[index, 'context1'] += sentences[position + 1]\n",
    "                \n",
    "                # mettre tout le speech dans le contexte 2\n",
    "                dataset_component.at[index, 'context2'] = speeches.iloc[0].Speech\n",
    "        else:\n",
    "            # plusieurs speeches contiennent le component\n",
    "            # il va falloir verifier l'id du speech et de la section pour verifier si c'est le bon\n",
    "            multiple_speech.append(index)\n",
    "    return not_found, multiple_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants test non trouvés: 44 -> \u001b[92m 0.463%\u001b[0m\n",
      "Nombre de composants test trouvés dans plusieurs discours: 1030 -> \u001b[91m 10.850%\u001b[0m\n",
      "\n",
      "Nombre de composants train non trouvés: 106 -> \u001b[92m 0.621%\u001b[0m\n",
      "Nombre de composants train trouvés dans plusieurs discours: 777 -> \u001b[93m 4.552%\u001b[0m\n",
      "\n",
      "Nombre de composants validation non trouvés: 36 -> \u001b[92m 0.560%\u001b[0m\n",
      "Nombre de composants validation trouvés dans plusieurs discours: 144 -> \u001b[93m 2.241%\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tester la fonction\n",
    "not_founded_test = []\n",
    "multiple_founded_test = []\n",
    "not_founded_test, multiple_founded_test = find1Component(test_components_data, test_speeches_data)\n",
    "print(\"Nombre de composants test non trouvés: {} -> {}\".format(len(not_founded_test), printResultColor(len(test_components_data), len(not_founded_test))))\n",
    "print(\"Nombre de composants test trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_test), printResultColor(len(test_components_data), len(multiple_founded_test))))\n",
    "\n",
    "not_founded_train = []\n",
    "multiple_founded_train = []\n",
    "not_founded_train, multiple_founded_train = find1Component(train_components_data, train_speeches_data)\n",
    "print(\"Nombre de composants train non trouvés: {} -> {}\".format(len(not_founded_train), printResultColor(len(train_components_data), len(not_founded_train))))\n",
    "print(\"Nombre de composants train trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_train), printResultColor(len(train_components_data), len(multiple_founded_train))))\n",
    "\n",
    "not_founded_validation = []\n",
    "multiple_founded_validation = []\n",
    "not_founded_validation, multiple_founded_validation = find1Component(validation_components_data, validation_speeches_data)\n",
    "print(\"Nombre de composants validation non trouvés: {} -> {}\".format(len(not_founded_validation), printResultColor(len(validation_components_data), len(not_founded_validation))))\n",
    "print(\"Nombre de composants validation trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_validation), printResultColor(len(validation_components_data), len(multiple_founded_validation))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still We can no longer afford to be second best\n",
      "['I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still.', 'We can no longer afford to be second best.']\n"
     ]
    }
   ],
   "source": [
    "# en affichant le text du premier component non trouvé, on se rend compte que tout le component est juste mais qu'il manque le point entre les deux phrases (d'où le fait que l'on ne le trouve pas)\n",
    "component_text = test_components_data.iloc[not_founded_test[0]].Text\n",
    "print(component_text)\n",
    "# print le speech test SectionID 9 SpeechID 1 de KENNEDY (id 9)\n",
    "speech_kennedy = test_speeches_data.iloc[6].Speech\n",
    "# tokenize le speech en phrases pour faciliter l'affichage\n",
    "sentences = sent_tokenize(speech_kennedy)\n",
    "# afficher les phrases 25 et 26\n",
    "print(sentences[25:27])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la deuxième fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous les components qui sont en plusieurs phrases et qui n'ont pas été trouvés précédements, avec ou sans la ponctuation. Le resultat renvoyé sera similaire à celui de la fonction précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find2Components(idsComponents, dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    # pour chaque id de component\n",
    "    for idComponent in idsComponents:\n",
    "        # get le text du component\n",
    "        textToFind = re.sub(r'[\\.!\\?]?( -)?', '', dataset_component.iloc[idComponent].Text)\n",
    "        # find the text in the speeches, but this time we remove the ponctuations of the speeches using replace function of pandas\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.replace(r'[\\.!\\?]?( -)?', '', regex=True).str.find(textToFind) != -1]\n",
    "        if(len(speeches) == 1):\n",
    "            if(idComponent == 187):\n",
    "                print(\"ici\")\n",
    "            speeches = speeches.iloc[0]\n",
    "            # si on a trouvé le component dans un seul speech\n",
    "            # tokenizer le component en faisant un split sur les espaces\n",
    "            words_component = dataset_component.iloc[idComponent].Text.split(\" \")\n",
    "            full_component = \"\"\n",
    "            for word in words_component:\n",
    "                if(speeches.Speech.find(full_component) != -1):\n",
    "                    # si le component est dans le texte\n",
    "                    # on ajoute le mot suivant au component\n",
    "                    if(len(full_component) > 0):\n",
    "                        full_component += \" \"+word\n",
    "                    else:\n",
    "                        full_component = word\n",
    "                else:\n",
    "                    # il faut enlever le dernier mot du component\n",
    "                    # car il manque surement une ponctuation\n",
    "                    last = full_component[full_component.rfind(\" \"):]\n",
    "                    # remove the last word from the component\n",
    "                    full_component = full_component[:full_component.rfind(\" \")]\n",
    "                    letter = speeches.Speech[speeches.Speech.find(full_component)+len(full_component)]\n",
    "                    if(speeches.Speech.find(full_component+letter+last) != -1):\n",
    "                        full_component += letter+last+\" \"+word\n",
    "                    elif(speeches.Speech.find(full_component+\" \"+letter+last) != -1):\n",
    "                        full_component += \" \"+letter+last+\" \"+word\n",
    "            # print the component before and after the correction in a file to see the difference\n",
    "            with open(\"components.txt\", \"a\") as file:\n",
    "                file.write(dataset_component.iloc[idComponent].Text+\"\\n\"+full_component+\"\\n\\n\")\n",
    "                \n",
    "        else:\n",
    "            not_found.append(idComponent)\n",
    "    print(\"Nombre de composants trouvés {} sur {}\".format(len(idsComponents) - len(not_found), len(idsComponents)))\n",
    "    return not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants trouvés 1 sur 106\n"
     ]
    }
   ],
   "source": [
    "not_fnd = find2Components(not_founded_train, train_components_data, train_speeches_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text index 603 of test component, dataset (id 603, Speech: <font color=\"red\">0</font>, Section: 8, Speaker: <font color=\"red\">Walters</font>):\n",
    "\n",
    "<font color=\"orange\">the tax program of the Ford administration will bring jobs where people are, and help to revitalize those cities as they can be</font>\n",
    "\n",
    "The component of the dataset is (id: 62 - speech <font color=\"green\">4</font> section 8 by <font color=\"green\">Ford</font>):\n",
    "\n",
    "(...) The net result is uh - we think under Carla Hills, who's the chairman of my uh - urban development and uh - neighborhood revitalization program, we will really do a first-class job in helping uh - the communities throughout the country. As a matter of fact, that committee under Secretary Hills released about a seventy-five-page report with specific recommendations so we can do a better job uh - the weeks ahead. And in addition, <font color=\"orange\">the tax program of the Ford administration</font><font color=\"red\">, which provides an incentive for industry to move into our major uh - metropolitan areas, into the inner cities,</font><font color=\"orange\"> will bring jobs where people are, and help to revitalize those cities as they can be</font>.\n",
    "\n",
    "**In this case, should we patch the Text of the dataset component with the punctuation and the missing part of the sentence or not?**\n",
    "\n",
    "**The speaker name as well as the section number is false, do we need to find an automatic way to correct these errors?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text index 187 of train component:\n",
    "\n",
    "<font color=\"orange\"> I don't think it was helpful when he suggested that seventeen million people go to bed hungry every night in the United States</font>\n",
    "\n",
    "The component of the dataset is uncorrected (id: 22 - speech 2 section 11 by Nixon):\n",
    "\n",
    "I did. And as I pointed out in 1952, (...) I don't think he should say that our prestige is at an all-time low. I think this is very harmful at a time Mr. Khrushchev is here - harmful because it's wrong. <font color=\"orange\">I don't think it was helpful when he suggested</font><font color=\"red\"> - and I'm glad he's corrected this to an extent - </font><font color=\"orange\">that seventeen million people go to bed hungry every night in the United States</font>. Now this just wasn't true. Now, there are people who go to bed hungry in the United States - far less, incidentally, than used to go to bed hungry when we came into power at the end of the Truman Administration. But the thing that is right about the United States, it should be emphasized, (...)\n",
    "\n",
    "\n",
    "**The component does not take into account all the text of the sentence. Do we must automatically patch it because it is a case that repeats itself quite frequently?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another example would be, if one of our components is between two sentences, can we delimitate as follows:**\n",
    "\n",
    "<font color=\"purple\">A donut is a type of pastry that is typically round with a hole in the middle. </font><font color=\"green\">It is usually fried and made from a sweet dough that can be flavored with </font><font color=\"orange\">various ingredients such as cinnamon or chocolate. Donuts are often served as a breakfast or dessert</font><font color=\"green\"> food and can be enjoyed plain or with toppings like frosting, sprinkles, or fruit.</font> <font color=\"blue\">In recent years, donuts have become a popular trend in the culinary world, with many bakeries offering unique and creative flavors and designs.</font> Despite their delicious taste, donuts should be consumed in moderation as they are high in sugar and calories.\n",
    "\n",
    "<font color=\"orange\">Component</font>: various ingredients such as cinnamon or chocolate. Donuts are often served as a breakfast or dessert\n",
    "\n",
    "<font color=\"green\">Current</font>: It is usually fried and made from a sweet dough that can be flavored with various ingredients such as cinnamon or chocolate. Donuts are often served as a breakfast or dessert food and can be enjoyed plain or with toppings like frosting, sprinkles, or fruit.\n",
    "\n",
    "<font color=\"purple\">Previous</font>: A donut is a type of pastry that is typically round with a hole in the middle. \n",
    "\n",
    "<font color=\"blue\">Next</font>: In recent years, donuts have become a popular trend in the culinary world, with many bakeries offering unique and creative flavors and designs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A strategy for finding components missing a piece of the original text would be to do:**\n",
    "\n",
    "1. transform the component into a bag of words\n",
    "\n",
    "2. make a first list of words from 0 to n words and another from n+1 to Len(component)-1 so both lists form the whole component\n",
    "\n",
    "3. list all the speeches that contain the first list and the second one, if there are no found speeches, then increment n by 1 and repeat step 2 (as long as n < len(component)-1)\n",
    "\n",
    "**This would be a conceivable technique ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to find 2 line with the same text\n",
    "# ^([^\\n]+)\\n\\1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
