{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire tous les imports nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants :  32988\n"
     ]
    }
   ],
   "source": [
    "# les components\n",
    "test_components_data = pd.read_csv('./data/test_components.csv')\n",
    "train_components_data = pd.read_csv('./data/train_components.csv')\n",
    "validation_components_data = pd.read_csv('./data/validation_components.csv')\n",
    "\n",
    "# ajouts des 2 colonnes de contexte aux components\n",
    "test_components_data['context1'] = ''\n",
    "test_components_data['context2'] = ''\n",
    "train_components_data['context1'] = ''\n",
    "train_components_data['context2'] = ''\n",
    "validation_components_data['context1'] = ''\n",
    "validation_components_data['context2'] = ''\n",
    "\n",
    "# les speechs\n",
    "test_speeches_data = pd.read_csv('./data/test_speeches.csv')\n",
    "train_speeches_data = pd.read_csv('./data/train_speeches.csv')\n",
    "validation_speeches_data = pd.read_csv('./data/validation_speeches.csv')\n",
    "\n",
    "# données de statistiques\n",
    "size_components_data = len(test_components_data) + len(train_components_data) + len(validation_components_data)\n",
    "print(\"Nombre de composants : \", size_components_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petite fonction pour les resultats intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction permettant de retourner string permettant un print en couleur sur un pourcentage en python\n",
    "# plus le pourcentage est proche de 0 plus il sera rouge, plus il est proche de 100 plus il sera vert\n",
    "# le pourcentage sera calculé en fonction de la taille du dataset et d'un chiffre donné\n",
    "def printResultColor(size_dataset, number):\n",
    "    pourcentage = number / size_dataset * 100\n",
    "    if pourcentage < 1:\n",
    "        # print vert\n",
    "        return(\"\\033[92m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    elif pourcentage < 5:\n",
    "        # print orange\n",
    "        return(\"\\033[93m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    else:\n",
    "        # print rouge\n",
    "        return(\"\\033[91m {0:0.3f}%\\033[0m\".format(pourcentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la premiere fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous components dans les Speeches. Une fois chaque Text trouvé, il faut completer le context 1 et 2.\n",
    "\n",
    "Cette fonction renvoie une liste de tous les identifiants des components qui n'ont pas été trouvés et une liste de tous les composants qui ont été trouvés plusieur fois (multiple speeches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find1Component(dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    multiple_speech = []\n",
    "    for index, row in dataset_component.iterrows():\n",
    "        textToFind = row.Text\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.find(textToFind) != -1]\n",
    "        # verifier que le component est bien dans un speech\n",
    "        if len(speeches) == 0:\n",
    "            not_found.append(index)\n",
    "        elif len(speeches) == 1:\n",
    "            # trouver le contexte 1 et 2\n",
    "            # tokenizer le speech à l'aide de nltk\n",
    "            sentences = sent_tokenize(speeches.iloc[0].Speech)\n",
    "            # trouver la position de la phrase contenant le component\n",
    "            position = -1\n",
    "            for i in range(len(sentences)):\n",
    "                if sentences[i].find(textToFind.strip()) != -1:\n",
    "                    position = i\n",
    "                    break\n",
    "            if position == -1:\n",
    "                # le component n'a pas été trouvé dans le speech car il est en plusieurs phrases\n",
    "                # ici nous allons donc faire un autre traitement pour trouver le contexte 1 et 2\n",
    "                # dans un premier temps nous allons tokenizer le component en phrases\n",
    "                component_sentences = sent_tokenize(textToFind)\n",
    "                # ensuite nous allons chercher la position de la phrase contenant le component\n",
    "                position = -1\n",
    "                positions = np.where([sentences[i].find(component_sentences[0].strip()) != -1 for i in range(len(sentences))])\n",
    "                for i in range(len(positions[0])):\n",
    "                    if sentences[positions[0][i]+1].find(component_sentences[1].strip()) != -1:\n",
    "                        position = positions[0][i]\n",
    "                        break\n",
    "                try:\n",
    "                    if(position != -1):\n",
    "                        # verifier que la suite du component est bien dans la phrase suivante\n",
    "                        if sentences[position + 1].find(component_sentences[1].strip()) != -1:\n",
    "                            # trouver le contexte 1\n",
    "                            if position > 0:\n",
    "                                dataset_component.at[index, 'context1'] = sentences[position - 1]\n",
    "                            # ajouter la phrase contenant le component au contexte 1\n",
    "                            dataset_component.at[index, 'context1'] += sentences[position]\n",
    "                            dataset_component.at[index, 'context1'] += sentences[position + 1]\n",
    "                            if position < len(sentences) - 2:\n",
    "                                # ajouter la phrase qui suit le component au contexte 1\n",
    "                                dataset_component.at[index, 'context1'] += sentences[position + 2]\n",
    "                            \n",
    "                            # mettre tout le speech dans le contexte 2\n",
    "                            dataset_component.at[index, 'context2'] = speeches.iloc[0].Speech\n",
    "                        else:\n",
    "                            print(\"La suite du component {} n'a pas été trouvés ?\".format(index))\n",
    "                    else:\n",
    "                        print(\"Le component {} n'a pas été trouvé en cherchant sur les phrases\".format(index))\n",
    "                except:\n",
    "                    print(\"Something went wrong with the component: {}\".format(textToFind))\n",
    "            else:\n",
    "                # trouver le contexte 1\n",
    "                if position > 0:\n",
    "                    dataset_component.at[index, 'context1'] = sentences[position - 1]\n",
    "                # ajouter la phrase contenant le component au contexte 1\n",
    "                dataset_component.at[index, 'context1'] += sentences[position]\n",
    "                # ajouter la phrase qui suit le component au contexte 1\n",
    "                if position < len(sentences) - 1:\n",
    "                    dataset_component.at[index, 'context1'] += sentences[position + 1]\n",
    "                \n",
    "                # mettre tout le speech dans le contexte 2\n",
    "                dataset_component.at[index, 'context2'] = speeches.iloc[0].Speech\n",
    "        else:\n",
    "            # plusieurs speeches contiennent le component\n",
    "            # il va falloir verifier l'id du speech et de la section pour verifier si c'est le bon\n",
    "            multiple_speech.append(index)\n",
    "    return not_found, multiple_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants test non trouvés: 44 -> \u001b[92m 0.463%\u001b[0m\n",
      "Nombre de composants test trouvés dans plusieurs discours: 1030 -> \u001b[91m 10.850%\u001b[0m\n",
      "\n",
      "Nombre de composants train non trouvés: 106 -> \u001b[92m 0.621%\u001b[0m\n",
      "Nombre de composants train trouvés dans plusieurs discours: 777 -> \u001b[93m 4.552%\u001b[0m\n",
      "\n",
      "Nombre de composants validation non trouvés: 36 -> \u001b[92m 0.560%\u001b[0m\n",
      "Nombre de composants validation trouvés dans plusieurs discours: 144 -> \u001b[93m 2.241%\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tester la fonction\n",
    "not_founded_test = []\n",
    "multiple_founded_test = []\n",
    "not_founded_test, multiple_founded_test = find1Component(test_components_data, test_speeches_data)\n",
    "print(\"Nombre de composants test non trouvés: {} -> {}\".format(len(not_founded_test), printResultColor(len(test_components_data), len(not_founded_test))))\n",
    "print(\"Nombre de composants test trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_test), printResultColor(len(test_components_data), len(multiple_founded_test))))\n",
    "\n",
    "not_founded_train = []\n",
    "multiple_founded_train = []\n",
    "not_founded_train, multiple_founded_train = find1Component(train_components_data, train_speeches_data)\n",
    "print(\"Nombre de composants train non trouvés: {} -> {}\".format(len(not_founded_train), printResultColor(len(train_components_data), len(not_founded_train))))\n",
    "print(\"Nombre de composants train trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_train), printResultColor(len(train_components_data), len(multiple_founded_train))))\n",
    "\n",
    "not_founded_validation = []\n",
    "multiple_founded_validation = []\n",
    "not_founded_validation, multiple_founded_validation = find1Component(validation_components_data, validation_speeches_data)\n",
    "print(\"Nombre de composants validation non trouvés: {} -> {}\".format(len(not_founded_validation), printResultColor(len(validation_components_data), len(not_founded_validation))))\n",
    "print(\"Nombre de composants validation trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_validation), printResultColor(len(validation_components_data), len(multiple_founded_validation))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still We can no longer afford to be second best\n",
      "['I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still.', 'We can no longer afford to be second best.']\n"
     ]
    }
   ],
   "source": [
    "# en affichant le text du premier component non trouvé, on se rend compte que tout le component est juste mais qu'il manque le point entre les deux phrases (d'où le fait que l'on ne le trouve pas)\n",
    "component_text = test_components_data.iloc[not_founded_test[0]].Text\n",
    "print(component_text)\n",
    "# print le speech test SectionID 9 SpeechID 1 de KENNEDY (id 9)\n",
    "speech_kennedy = test_speeches_data.iloc[6].Speech\n",
    "# tokenize le speech en phrases pour faciliter l'affichage\n",
    "sentences = sent_tokenize(speech_kennedy)\n",
    "# afficher les phrases 25 et 26\n",
    "print(sentences[25:27])\n",
    "\n",
    "# Cet exemple montre la pertinence de la fonction suivante"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la deuxième fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous les components qui sont en plusieurs phrases et qui n'ont pas été trouvés précédements, avec ou sans la ponctuation. Le resultat renvoyé sera similaire à celui de la fonction précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find2Components(idsComponents, dataset_component, dataset_speeches):\n",
    "#     not_found = []\n",
    "#     # pour chaque id de component\n",
    "#     for idComponent in idsComponents:\n",
    "#         # get le text du component\n",
    "#         textToFind = re.sub(r'[\\.!\\?]?( -)?', '', dataset_component.iloc[idComponent].Text)\n",
    "#         # find the text in the speeches, but this time we remove the ponctuations of the speeches using replace function of pandas\n",
    "#         speeches = dataset_speeches[dataset_speeches['Speech'].str.replace(r'[\\.!\\?]?( -)?', '', regex=True).str.find(textToFind) != -1]\n",
    "#         if(len(speeches) == 1):\n",
    "#             if(idComponent == 187):\n",
    "#                 print(\"ici\")\n",
    "#             speeches = speeches.iloc[0]\n",
    "#             # si on a trouvé le component dans un seul speech\n",
    "#             # tokenizer le component en faisant un split sur les espaces\n",
    "#             words_component = dataset_component.iloc[idComponent].Text.split(\" \")\n",
    "#             full_component = \"\"\n",
    "#             for word in words_component:\n",
    "#                 if(speeches.Speech.find(full_component) != -1):\n",
    "#                     # si le component est dans le texte\n",
    "#                     # on ajoute le mot suivant au component\n",
    "#                     if(len(full_component) > 0):\n",
    "#                         full_component += \" \"+word\n",
    "#                     else:\n",
    "#                         full_component = word\n",
    "#                 else:\n",
    "#                     # il faut enlever le dernier mot du component\n",
    "#                     # car il manque surement une ponctuation\n",
    "#                     last = full_component[full_component.rfind(\" \"):]\n",
    "#                     # remove the last word from the component\n",
    "#                     full_component = full_component[:full_component.rfind(\" \")]\n",
    "#                     letter = speeches.Speech[speeches.Speech.find(full_component)+len(full_component)]\n",
    "#                     if(speeches.Speech.find(full_component+letter+last) != -1):\n",
    "#                         full_component += letter+last+\" \"+word\n",
    "#                     elif(speeches.Speech.find(full_component+\" \"+letter+last) != -1):\n",
    "#                         full_component += \" \"+letter+last+\" \"+word\n",
    "#             # print the component before and after the correction in a file to see the difference\n",
    "#             with open(\"components.txt\", \"a\") as file:\n",
    "#                 file.write(dataset_component.iloc[idComponent].Text+\"\\n\"+full_component+\"\\n\\n\")\n",
    "                \n",
    "#         else:\n",
    "#             not_found.append(idComponent)\n",
    "#     print(\"Nombre de composants trouvés {} sur {}\".format(len(idsComponents) - len(not_found), len(idsComponents)))\n",
    "#     return not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m                     \u001b[39m# plusieurs résultats, cas particulier à traiter\u001b[39;00m\n\u001b[0;32m     28\u001b[0m                     not_found\u001b[39m.\u001b[39mappend(idComponent)\n\u001b[1;32m---> 29\u001b[0m not_fnd \u001b[39m=\u001b[39m find3Components(not_founded_test, test_components_data, test_speeches_data)\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mfind3Components\u001b[1;34m(idsComponents, dataset_component, dataset_speeches)\u001b[0m\n\u001b[0;32m     17\u001b[0m     speeches_intermed \u001b[39m=\u001b[39m speeches_intermed[speeches_intermed[\u001b[39m'\u001b[39m\u001b[39mSpeech\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mfind(end_component) \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(speeches_intermed) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     19\u001b[0m         \u001b[39m# append each speech that contains the component\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         speeches\u001b[39m.\u001b[39;49mappend(speeches_intermed)\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(speeches)\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(speeches) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5987\u001b[0m ):\n\u001b[0;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "def find3Components(idsComponents, dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    # pour chaque id de component\n",
    "    for idComponent in idsComponents:\n",
    "        print(idComponent)\n",
    "        # split le component en mots using nltk\n",
    "        words_component = word_tokenize(dataset_component.iloc[idComponent].Text)\n",
    "        if idComponent == 56:\n",
    "            # speeches is a list of rows of speeches that contain the component\n",
    "            speeches = []\n",
    "            for n in range(1, len(words_component)):\n",
    "                # concat les n premiers mots du component\n",
    "                start_component = \" \".join(words_component[0:n])\n",
    "                end_component = \" \".join(words_component[n:])\n",
    "                # get all the speeches that contain the start component and the end component\n",
    "                speeches_intermed = dataset_speeches[dataset_speeches['Speech'].str.find(start_component) != -1]\n",
    "                speeches_intermed = speeches_intermed[speeches_intermed['Speech'].str.find(end_component) != -1]\n",
    "                if(len(speeches_intermed) > 0):\n",
    "                    # append each speech that contains the component\n",
    "                    speeches.append(speeches_intermed)\n",
    "            print(speeches)\n",
    "            if(len(speeches) > 0):\n",
    "                if(len(speeches) == 1):\n",
    "                    # on l'a surrement trouvé\n",
    "                    print(speeches[0])\n",
    "                else:\n",
    "                    # plusieurs résultats, cas particulier à traiter\n",
    "                    not_found.append(idComponent)\n",
    "not_fnd = find3Components(not_founded_test, test_components_data, test_speeches_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text index 603 of test component, dataset (id 603, Speech: <font color=\"red\">0</font>, Section: 8, Speaker: <font color=\"red\">Walters</font>):\n",
    "\n",
    "<font color=\"orange\">the tax program of the Ford administration will bring jobs where people are, and help to revitalize those cities as they can be</font>\n",
    "\n",
    "The component of the dataset is (id: 62 - speech <font color=\"green\">4</font> section 8 by <font color=\"green\">Ford</font>):\n",
    "\n",
    "(...) The net result is uh - we think under Carla Hills, who's the chairman of my uh - urban development and uh - neighborhood revitalization program, we will really do a first-class job in helping uh - the communities throughout the country. As a matter of fact, that committee under Secretary Hills released about a seventy-five-page report with specific recommendations so we can do a better job uh - the weeks ahead. And in addition, <font color=\"orange\">the tax program of the Ford administration</font><font color=\"red\">, which provides an incentive for industry to move into our major uh - metropolitan areas, into the inner cities,</font><font color=\"orange\"> will bring jobs where people are, and help to revitalize those cities as they can be</font>.\n",
    "\n",
    "**In this case, should we patch the Text of the dataset component with the punctuation and the missing part of the sentence or not?**\n",
    "\n",
    "**The speaker name as well as the section number is false, do we need to find an automatic way to correct these errors?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text index 187 of train component:\n",
    "\n",
    "<font color=\"orange\"> I don't think it was helpful when he suggested that seventeen million people go to bed hungry every night in the United States</font>\n",
    "\n",
    "The component of the dataset is uncorrected (id: 22 - speech 2 section 11 by Nixon):\n",
    "\n",
    "I did. And as I pointed out in 1952, (...) I don't think he should say that our prestige is at an all-time low. I think this is very harmful at a time Mr. Khrushchev is here - harmful because it's wrong. <font color=\"orange\">I don't think it was helpful when he suggested</font><font color=\"red\"> - and I'm glad he's corrected this to an extent - </font><font color=\"orange\">that seventeen million people go to bed hungry every night in the United States</font>. Now this just wasn't true. Now, there are people who go to bed hungry in the United States - far less, incidentally, than used to go to bed hungry when we came into power at the end of the Truman Administration. But the thing that is right about the United States, it should be emphasized, (...)\n",
    "\n",
    "\n",
    "**The component does not take into account all the text of the sentence. Do we must automatically patch it because it is a case that repeats itself quite frequently?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another example would be, if one of our components is between two sentences, can we delimitate as follows:**\n",
    "\n",
    "<font color=\"purple\">A donut is a type of pastry that is typically round with a hole in the middle. </font><font color=\"green\">It is usually fried and made from a sweet dough that can be flavored with </font><font color=\"orange\">various ingredients such as cinnamon or chocolate. Donuts are often served as a breakfast or dessert</font><font color=\"green\"> food and can be enjoyed plain or with toppings like frosting, sprinkles, or fruit.</font> <font color=\"blue\">In recent years, donuts have become a popular trend in the culinary world, with many bakeries offering unique and creative flavors and designs.</font> Despite their delicious taste, donuts should be consumed in moderation as they are high in sugar and calories.\n",
    "\n",
    "<font color=\"orange\">Component</font>: various ingredients such as cinnamon or chocolate. Donuts are often served as a breakfast or dessert\n",
    "\n",
    "<font color=\"green\">Current</font>: It is usually fried and made from a sweet dough that can be flavored with various ingredients such as cinnamon or chocolate. Donuts are often served as a breakfast or dessert food and can be enjoyed plain or with toppings like frosting, sprinkles, or fruit.\n",
    "\n",
    "<font color=\"purple\">Previous</font>: A donut is a type of pastry that is typically round with a hole in the middle. \n",
    "\n",
    "<font color=\"blue\">Next</font>: In recent years, donuts have become a popular trend in the culinary world, with many bakeries offering unique and creative flavors and designs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la troisième fonction\n",
    "\n",
    "Cette fonction va chercher tous les composants qui ont été trouvés dans plusieurs speechs et verifier si il est possible de trouver le speech correspondant à notre composant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find3Components(idsComponents, dataset_component, dataset_speeches):\n",
    "    # first we need to create an empty array wich will contain the components that we will not find\n",
    "    not_found = []\n",
    "    not_found_multiple = []\n",
    "    # next we need to iterate over each component\n",
    "    for idComponent in idsComponents:\n",
    "        # get the component\n",
    "        component = dataset_component.iloc[idComponent]\n",
    "        # get the text of the component\n",
    "        textToFind = component.Text\n",
    "        # get all the speeches that contain the component and wich have the same SpeechID and SectionID\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.find(textToFind) != -1]\n",
    "        speeches = speeches[speeches['SpeechID'] == component.SpeechID]\n",
    "        speeches = speeches[speeches['SectionID'] == component.SectionID]\n",
    "        # if we find the component in one speech\n",
    "        if(len(speeches) == 1):\n",
    "            # we find the right speech\n",
    "            # get the speech text\n",
    "            speech = speeches.iloc[0].Speech\n",
    "            sentences = sent_tokenize(speech)\n",
    "            # trouver la position de la phrase contenant le component\n",
    "            position = -1\n",
    "            for i in range(len(sentences)):\n",
    "                if sentences[i].find(textToFind.strip()) != -1:\n",
    "                    position = i\n",
    "                    break\n",
    "            # si on a trouvé la position de la phrase\n",
    "            if(position != -1):\n",
    "                if position > 0:\n",
    "                    dataset_component.at[idComponent, 'context1'] = sentences[position - 1]\n",
    "                # ajouter la phrase contenant le component au contexte 1\n",
    "                dataset_component.at[idComponent, 'context1'] += sentences[position]\n",
    "                # ajouter la phrase qui suit le component au contexte 1\n",
    "                if position < len(sentences) - 1:\n",
    "                    dataset_component.at[idComponent, 'context1'] += sentences[position + 1]\n",
    "                \n",
    "                # mettre tout le speech dans le contexte 2\n",
    "                dataset_component.at[idComponent, 'context2'] = speech\n",
    "            else:\n",
    "                # print(\"position not found for {}\".format(idComponent))\n",
    "                not_found.append(idComponent)\n",
    "            \n",
    "        elif(len(speeches) > 1):\n",
    "            # this technique is not enough to find the component\n",
    "            not_found_multiple.append(idComponent)\n",
    "        else:\n",
    "            # we simply not found the component with this technique, mabe the component SectionID or SpeechID is wrong\n",
    "            not_found.append(idComponent)\n",
    "    return not_found, not_found_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants test multiple non trouvés: 9 sur les 1030 -> \u001b[92m 0.874%\u001b[0m\n",
      "Nombre de composants test multiple non trouvés dans qu'un seul speech: 10 sur les 1030 -> \u001b[92m 0.971%\u001b[0m\n",
      "\n",
      "Nombre de composants train multiple non trouvés: 10 sur les 777 -> \u001b[93m 1.287%\u001b[0m\n",
      "Nombre de composants train multiple non trouvés dans qu'un seul speech: 21 sur les 777 -> \u001b[93m 2.703%\u001b[0m\n",
      "\n",
      "Nombre de composants validation multiple non trouvés: 0 sur les 144 -> \u001b[92m 0.000%\u001b[0m\n",
      "Nombre de composants validation multiple non trouvés dans qu'un seul speech: 4 sur les 144 -> \u001b[93m 2.778%\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "not_found_multi_test, not_found_in_one_test = find3Components(multiple_founded_test, test_components_data, test_speeches_data)\n",
    "print(\"Nombre de composants test multiple non trouvés: {} sur les {} -> {}\".format(len(not_found_multi_test), len(multiple_founded_test), printResultColor(len(multiple_founded_test), len(not_found_multi_test))))\n",
    "print(\"Nombre de composants test multiple non trouvés dans qu'un seul speech: {} sur les {} -> {}\\n\".format(len(not_found_in_one_test), len(multiple_founded_test), printResultColor(len(multiple_founded_test), len(not_found_in_one_test))))\n",
    "\n",
    "not_found_multi_train, not_found_in_one_train = find3Components(multiple_founded_train, train_components_data, train_speeches_data)\n",
    "print(\"Nombre de composants train multiple non trouvés: {} sur les {} -> {}\".format(len(not_found_multi_train), len(multiple_founded_train), printResultColor(len(multiple_founded_train), len(not_found_multi_train))))\n",
    "print(\"Nombre de composants train multiple non trouvés dans qu'un seul speech: {} sur les {} -> {}\\n\".format(len(not_found_in_one_train), len(multiple_founded_train), printResultColor(len(multiple_founded_train), len(not_found_in_one_train))))\n",
    "\n",
    "not_found_multi_validation, not_found_in_one_validation = find3Components(multiple_founded_validation, validation_components_data, validation_speeches_data)\n",
    "print(\"Nombre de composants validation multiple non trouvés: {} sur les {} -> {}\".format(len(not_found_multi_validation), len(multiple_founded_validation), printResultColor(len(multiple_founded_validation), len(not_found_multi_validation))))\n",
    "print(\"Nombre de composants validation multiple non trouvés dans qu'un seul speech: {} sur les {} -> {}\\n\".format(len(not_found_in_one_validation), len(multiple_founded_validation), printResultColor(len(multiple_founded_validation), len(not_found_in_one_validation))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7265\n",
      "Int64Index([1395, 1495], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# print un élément non trouvé de la liste not_found_multi_train\n",
    "print(not_found_multi_train[0])\n",
    "# get les ids des speeches qui contiennent le component\n",
    "speeches = train_speeches_data[train_speeches_data['Speech'].str.find(train_components_data.iloc[not_found_multi_train[0]].Text) != -1]\n",
    "print(speeches.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to find 2 line with the same text\n",
    "# ^([^\\n]+)\\n\\1$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID 622 et 582 de la liste speech test sont exactement les mêmes sauf leurs Section ID  (21_1992 et 9)\n",
    "\n",
    "Le premier cas est il un ID + une date ? (21_1992)\n",
    "\n",
    "Dans les deux cas, pourquoi nous avons des tuples dans notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants test sans contexte 1 et 2: 80\n",
      "Pourcecentage de composants test sans contexte 1 et 2: 0.8427262193194985%\n",
      "Nombre de composants train sans contexte 1 et 2: 137\n",
      "Pourcecentage de composants train sans contexte 1 et 2: 0.8026246411623411%\n",
      "Nombre de composants validation sans contexte 1 et 2: 40\n",
      "Pourcecentage de composants validation sans contexte 1 et 2: 0.6224712107065048%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17948\\1113298283.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  nb = len(test_components_data[train_components_data['context1'] == \"\"])\n"
     ]
    }
   ],
   "source": [
    "# get le nombre de component qui n'ont pas de conexte 1 et 2\n",
    "nb = len(test_components_data[train_components_data['context1'] == \"\"])\n",
    "print(\"Nombre de composants test sans contexte 1 et 2: {}\".format(nb))\n",
    "print(\"Pourcecentage de composants test sans contexte 1 et 2: {}%\".format(nb / len(test_components_data) * 100))\n",
    "\n",
    "nb_train = len(train_components_data[train_components_data['context1'] == \"\"])\n",
    "print(\"Nombre de composants train sans contexte 1 et 2: {}\".format(nb_train))\n",
    "print(\"Pourcecentage de composants train sans contexte 1 et 2: {}%\".format(nb_train / len(train_components_data) * 100))\n",
    "\n",
    "nb_validation = len(validation_components_data[validation_components_data['context1'] == \"\"])\n",
    "print(\"Nombre de composants validation sans contexte 1 et 2: {}\".format(nb_validation))\n",
    "print(\"Pourcecentage de composants validation sans contexte 1 et 2: {}%\".format(nb_validation / len(validation_components_data) * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
