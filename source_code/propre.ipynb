{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire tous les imports nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants :  32988\n"
     ]
    }
   ],
   "source": [
    "# les components\n",
    "test_components_data = pd.read_csv('./data/test_components.csv')\n",
    "train_components_data = pd.read_csv('./data/train_components.csv')\n",
    "validation_components_data = pd.read_csv('./data/validation_components.csv')\n",
    "\n",
    "# ajouts des 2 colonnes de contexte aux components\n",
    "test_components_data['context1'] = ''\n",
    "test_components_data['context2'] = ''\n",
    "train_components_data['context1'] = ''\n",
    "train_components_data['context2'] = ''\n",
    "validation_components_data['context1'] = ''\n",
    "validation_components_data['context2'] = ''\n",
    "\n",
    "# les speechs\n",
    "test_speeches_data = pd.read_csv('./data/test_speeches.csv')\n",
    "train_speeches_data = pd.read_csv('./data/train_speeches.csv')\n",
    "validation_speeches_data = pd.read_csv('./data/validation_speeches.csv')\n",
    "\n",
    "# données de statistiques\n",
    "size_components_data = len(test_components_data) + len(train_components_data) + len(validation_components_data)\n",
    "print(\"Nombre de composants : \", size_components_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petite fonction pour les resultats intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction permettant de retourner string permettant un print en couleur sur un pourcentage en python\n",
    "# plus le pourcentage est proche de 0 plus il sera rouge, plus il est proche de 100 plus il sera vert\n",
    "# le pourcentage sera calculé en fonction de la taille du dataset et d'un chiffre donné\n",
    "def printResultColor(size_dataset, number):\n",
    "    pourcentage = number / size_dataset * 100\n",
    "    if pourcentage < 1:\n",
    "        # print vert\n",
    "        return(\"\\033[92m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    elif pourcentage < 5:\n",
    "        # print orange\n",
    "        return(\"\\033[93m {0:0.3f}%\\033[0m\".format(pourcentage))\n",
    "    else:\n",
    "        # print rouge\n",
    "        return(\"\\033[91m {0:0.3f}%\\033[0m\".format(pourcentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la premiere fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous components dans les Speeches. Une fois chaque Text trouvé, il faut completer le context 1 et 2.\n",
    "\n",
    "Cette fonction renvoie une liste de tous les identifiants des components qui n'ont pas été trouvés et une liste de tous les composants qui ont été trouvés plusieur fois (multiple speeches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find1Component(dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    multiple_speech = []\n",
    "    for index, row in dataset_component.iterrows():\n",
    "        textToFind = row.Text\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.find(textToFind) != -1]\n",
    "        # verifier que le component est bien dans un speech\n",
    "        if len(speeches) == 0:\n",
    "            not_found.append(index)\n",
    "        elif len(speeches) == 1:\n",
    "            # trouver le contexte 1 et 2\n",
    "            # tokenizer le speech à l'aide de nltk\n",
    "            sentences = sent_tokenize(speeches.iloc[0].Speech)\n",
    "            # trouver la position de la phrase contenant le component\n",
    "            position = -1\n",
    "            for i in range(len(sentences)):\n",
    "                if sentences[i].find(textToFind) != -1:\n",
    "                    position = i\n",
    "                    break\n",
    "            if position == -1:\n",
    "                # peut arriver si le component est en plusieurs phrases et donc la tokenizer ne le trouve pas\n",
    "                not_found.append(index)\n",
    "            else:\n",
    "                # trouver le contexte 1\n",
    "                if position > 0:\n",
    "                    dataset_component.at[index, 'context1'] = sentences[position - 1]\n",
    "                # ajouter la phrase contenant le component au contexte 1\n",
    "                dataset_component.at[index, 'context1'] += sentences[position]\n",
    "                # ajouter la phrase qui suit le component au contexte 1\n",
    "                if position < len(sentences) - 1:\n",
    "                    dataset_component.at[index, 'context1'] += sentences[position + 1]\n",
    "                \n",
    "                # mettre tout le speech dans le contexte 2\n",
    "                dataset_component.at[index, 'context2'] = speeches.iloc[0].Speech\n",
    "        else:\n",
    "            # plusieurs speeches contiennent le component\n",
    "            # il va falloir verifier l'id du speech et de la section pour verifier si c'est le bon\n",
    "            multiple_speech.append(index)\n",
    "    return not_found, multiple_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de composants test non trouvés: 102 -> \u001b[93m 1.074%\u001b[0m\n",
      "Nombre de composants test trouvés dans plusieurs discours: 1030 -> \u001b[91m 10.850%\u001b[0m\n",
      "\n",
      "Nombre de composants train non trouvés: 306 -> \u001b[93m 1.793%\u001b[0m\n",
      "Nombre de composants train trouvés dans plusieurs discours: 777 -> \u001b[93m 4.552%\u001b[0m\n",
      "\n",
      "Nombre de composants validation non trouvés: 109 -> \u001b[93m 1.696%\u001b[0m\n",
      "Nombre de composants validation trouvés dans plusieurs discours: 144 -> \u001b[93m 2.241%\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tester la fonction\n",
    "not_founded_test = []\n",
    "multiple_founded_test = []\n",
    "not_founded_test, multiple_founded_test = find1Component(test_components_data, test_speeches_data)\n",
    "print(\"Nombre de composants test non trouvés: {} -> {}\".format(len(not_founded), printResultColor(len(test_components_data), len(not_founded))))\n",
    "print(\"Nombre de composants test trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded), printResultColor(len(test_components_data), len(multiple_founded))))\n",
    "\n",
    "not_founded_train = []\n",
    "multiple_founded_train = []\n",
    "not_founded_train, multiple_founded_train = find1Component(train_components_data, train_speeches_data)\n",
    "print(\"Nombre de composants train non trouvés: {} -> {}\".format(len(not_founded_train), printResultColor(len(train_components_data), len(not_founded_train))))\n",
    "print(\"Nombre de composants train trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_train), printResultColor(len(train_components_data), len(multiple_founded_train))))\n",
    "\n",
    "not_founded_validation = []\n",
    "multiple_founded_validation = []\n",
    "not_founded_validation, multiple_founded_validation = find1Component(validation_components_data, validation_speeches_data)\n",
    "print(\"Nombre de composants validation non trouvés: {} -> {}\".format(len(not_founded_validation), printResultColor(len(validation_components_data), len(not_founded_validation))))\n",
    "print(\"Nombre de composants validation trouvés dans plusieurs discours: {} -> {}\\n\".format(len(multiple_founded_validation), printResultColor(len(validation_components_data), len(multiple_founded_validation))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la deuxième fonction\n",
    "\n",
    "Cette fonction doit trouver le Text de tous les components qui sont en plusieurs phrases et qui n'ont pas été trouvés précédements, avec ou sans la ponctuation. Le resultat renvoyé sera similaire à celui de la fonction précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find2Components(idsComponents, dataset_component, dataset_speeches):\n",
    "    not_found = []\n",
    "    # pour chaque id de component\n",
    "    for idComponent in idsComponents:\n",
    "        # get le text du component\n",
    "        textToFind = dataset_component.iloc[idComponent].Text\n",
    "        # enlever les ponctuations\n",
    "        textToFind = re.sub(r'[\\.!\\?]','',textToFind)\n",
    "        # find the text in the speeches, but this time we remove the ponctuations of the speeches using replace function of pandas\n",
    "        speeches = dataset_speeches[dataset_speeches['Speech'].str.replace(r'[\\.!\\?]', '', regex=True).str.find(textToFind) != -1]\n",
    "        if(len(speeches) == 1):\n",
    "            # si on a trouvé le component dans un seul speech\n",
    "            # TODO pour cet après midi\n",
    "            pass\n",
    "        else:\n",
    "            not_found.append(idComponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "find2Components(not_founded_train, train_components_data, train_speeches_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
