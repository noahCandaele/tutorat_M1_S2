{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorat M1 S2 Informatique DS4H\n",
    "\n",
    "### Subject:\n",
    "\n",
    "#### Re-structuration intelligente d'un jeu de donnée de debats politiques pour l'extraction de structures argumentaires\n",
    "\n",
    "L'objectif du travail à réaliser est de structurer des données nécessaires à l'étude des composantes et des relations au sein des débats politiques qui ont eu lieu lors des élections du président des États-Unis de 1960 à 2016. Les débats se présentent sous la forme d'un dialogue entre un candidat et l'autre, qui répondent aux questions posées par un orateur sur divers sujets tels que l'économie, la sécurité, l'éducation, la guerre, les soins de santé, etc. Chaque débat a été divisé en sections en tenant compte des différents sujets abordés.\n",
    "\n",
    "Tous les débats ont été annotés d'un point de vue argumentatif. Des annotations concernant les composantes argumentatives sont présentes à l'intérieur :\n",
    "- Conclusions\n",
    "- Prémisses\n",
    "et des annotations faisant référence aux relations entre ces deux composants :\n",
    "- Attaque\n",
    "- Soutien\n",
    "- Équivalent\n",
    "\n",
    "L'objectif du projet est de concevoir et implémenter une structure de données textuels qui soit facile à manipuler pour la réalisation d'une des nombreuses tâches du TAL, à savoir l'extraction d'arguments.\n",
    "Plus précisément, il s’agit de deux structures :\n",
    "1) Un jeu de données référençant les composants (Claim, Premise) représentés par les colonnes suivantes :\n",
    "- Ligne de dialogue\n",
    "- Composants de l'argumentation\n",
    "- Schéma BIO des composants\n",
    "- Caractéristiques linguistiques, lexicales, grammaticales, syntaxiques, etc... (Chaque caractéristique séparée par une colonne) concernant le composant considéré\n",
    "2) Un ensemble de données se référant aux relations (Attaque, Soutien, Équivalent) regroupées par section et représentées par les colonnes suivantes :\n",
    "- Composante 1 (Claim/Premise)\n",
    "- Composante 2 (Claim/Premise)\n",
    "- Type de relation (Attaque/Soutien/Équivalent)\n",
    "- Schéma BIO des composants et des relations avec leur distance\n",
    "- Caractéristiques linguistiques, lexicales, grammaticales, syntaxiques, etc. (chaque caractéristique séparée par une colonne) concernant la relation considérée"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "* What if the component is two sentences ? Because we don't have any ponctuation in the current component dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data file\n",
    "components_data = pd.read_csv('./data/test_components.csv')\n",
    "speeches_data = pd.read_csv('./data/test_speeches.csv')\n",
    "# work with only the first 5 rows\n",
    "# components_data = components_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe that copy the components_data but without the \"Previous_Sentence\" and \"Next_Sentence\" columns\n",
    "components_data_context = components_data.drop(['Current_Sentence', 'Previous_Sentence', 'Next_Sentence'], axis=1)\n",
    "# add columns for contexts\n",
    "components_data_context['Context1'] = ''\n",
    "components_data_context['Context2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component not found in speeches: 56\n",
      "Component not found in speeches: 75\n",
      "Component not found in speeches: 94\n",
      "Component not found in speeches: 95\n",
      "Component not found in speeches: 603\n",
      "Component not found in speeches: 927\n",
      "Component not found in speeches: 935\n",
      "Component not found in speeches: 1335\n",
      "Component not found in speeches: 1410\n",
      "Component not found in speeches: 1416\n",
      "Component not found in speeches: 1429\n",
      "Component not found in speeches: 1525\n",
      "Component not found in speeches: 1709\n",
      "Component not found in speeches: 3552\n",
      "Component not found in speeches: 4821\n",
      "Component not found in speeches: 5416\n",
      "Component not found in speeches: 5434\n",
      "Component not found in speeches: 5521\n",
      "Component not found in speeches: 5537\n",
      "Component not found in speeches: 5579\n",
      "Component not found in speeches: 5583\n",
      "Component not found in speeches: 5601\n",
      "Component not found in speeches: 5644\n",
      "Component not found in speeches: 5650\n",
      "Component not found in speeches: 5757\n",
      "Component not found in speeches: 5817\n",
      "Component not found in speeches: 6118\n",
      "Component not found in speeches: 6124\n",
      "Component not found in speeches: 6188\n",
      "Component not found in speeches: 6208\n",
      "Component not found in speeches: 6806\n",
      "Component not found in speeches: 7325\n",
      "Component not found in speeches: 7582\n",
      "Component not found in speeches: 7590\n",
      "Component not found in speeches: 8316\n",
      "Component not found in speeches: 8573\n",
      "Component not found in speeches: 8827\n",
      "Component not found in speeches: 8850\n",
      "Component not found in speeches: 8932\n",
      "Component not found in speeches: 8963\n",
      "Component not found in speeches: 8981\n",
      "Component not found in speeches: 9026\n",
      "Component not found in speeches: 9027\n",
      "Component not found in speeches: 9029\n"
     ]
    }
   ],
   "source": [
    "# tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found = []\n",
    "\n",
    "# for each component, find the speeches that mention it\n",
    "for index, row in components_data.iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.find(textToFind) != -1]\n",
    "    \n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        # tokenize the speech into sentences\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        \n",
    "        context1 = ''\n",
    "        # get the sentence that contains the component\n",
    "        for index, sentence in zip(range(0,len(sentences)), sentences):\n",
    "            if sentence.find(textToFind) != -1:\n",
    "                if(index > 0):\n",
    "                    context1 = sentences[index-1]\n",
    "                # add the current sentence\n",
    "                context1 = context1 + ' ' + sentence\n",
    "                if(index < len(sentences)-1):\n",
    "                    context1 = context1 + ' ' + sentences[index+1]\n",
    "                break\n",
    "        # add context1\n",
    "        components_data_context.loc[index, 'Context1'] = context1\n",
    "        # add full speech to the context2 column\n",
    "        components_data_context.loc[index, 'Context2'] = speeches['Speech'].values[0]\n",
    "    except:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found.append(index)\n",
    "        print('Component not found in speeches: ' + str(index))\n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un certain nombre de composants n'ont pas été trouvés dans le texte, cela peut être du au format dans lesquels les données ont été enregistrées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of components not found: 0.4634994206257242%\n"
     ]
    }
   ],
   "source": [
    "# Un certain nombre de components n'ont pas été trouvés dans les speeches\n",
    "print('Percentage of components not found: ' + str(len(components_not_found)/len(components_data)*100) + '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suite va donc consister à détecter les différents formats pour pouvoir les patcher et les ajouter à notre premier contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still We can no longer afford to be second best\n"
     ]
    }
   ],
   "source": [
    "# prenons pour exemple le premier component qui n'a pas été trouvé\n",
    "cps = components_data.loc[components_not_found[0]]\n",
    "print(cps.Text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici le composent (id 56) est composé de 2 phrases, or la fonction sent_tokenize ne les détecte pas comme 2 phrases car il a été stocké sans la ponctuation.\n",
    "\n",
    "On va donc refaire une boucle mais cette fois sans la ponctuation pour détecter les phrases.\n",
    "(petit soucis avec cette technique, les phrases contenant les ' ne sont pas détectées, on va donc pas la suite essayer de trouver une autre solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vider le tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found_2 = []\n",
    "\n",
    "#loop through the components_data that have not been found\n",
    "for index, row in components_data.loc[components_not_found].iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text but this time we remove the ponctuation\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.translate(str.maketrans('', '', string.punctuation)).str.find(textToFind) != -1]\n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        \n",
    "        component = \"\"\n",
    "        first_id = -1\n",
    "        last_id = -1\n",
    "        # for each sentence of the speech\n",
    "        for sentence in sentences:\n",
    "            # if the sentence is in the component text\n",
    "            if textToFind.find(sentence.translate(str.maketrans('', '', string.punctuation))) != -1:\n",
    "                if first_id == -1:\n",
    "                    first_id = sentences.index(sentence)\n",
    "                last_id = sentences.index(sentence)\n",
    "                # concat the sentence to the component\n",
    "                component = component + ' ' + sentence\n",
    "        # update the component text\n",
    "        components_data.loc[index, 'Text'] = component\n",
    "        # update the component text in the components_data_context\n",
    "        components_data_context.loc[index, 'Text'] = component\n",
    "        # add the previous sentence\n",
    "        context1 = ''\n",
    "        if first_id > 0:\n",
    "            context1 += sentences[first_id-1]\n",
    "        context1 += ' ' + component\n",
    "        if last_id < len(sentences)-1:\n",
    "            context1 += ' ' + sentences[last_id+1]\n",
    "        # update the context1\n",
    "        components_data_context.loc[index, 'Context1'] = context1\n",
    "        # update the context2\n",
    "        components_data_context.loc[index, 'Context2'] = speeches['Speech'].values[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found_2.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechID                                                     3\n",
      "SectionID                                                    9\n",
      "Text         Anybody that says America has been standing st...\n",
      "Name: 75, dtype: object\n",
      "Percentage of components not found: 0.44243126514273673%\n"
     ]
    }
   ],
   "source": [
    "# get first component that has not been found\n",
    "comp = components_data.loc[components_not_found_2[0]]\n",
    "# print the speechid and the section id and the text of the component\n",
    "print(comp[['SpeechID', 'SectionID', 'Text']])\n",
    "# print the percentage of components that have not been found\n",
    "print('Percentage of components not found: ' + str(len(components_not_found_2)/len(components_data)*100) + '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, le pourcentage n'a pas beaucoup changé, on va donc essayer de trouver une autre solution.\n",
    "Le principal soucis ici est que les phrases contenant des ' ne sont pas détectées. Ce qui est un cas fréquent en anglais. Il faut donc que l'on trouve une solution pour détecter ces phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anybody that says America has been standing still for the last seven and a half years hasn't been traveling in America. He's been in some other country!\n",
      "Anybody that says America has been standing still for the last seven and a half years hasnt been traveling in America Hes been in some other country\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Anybody that says America has been standing still for the last seven and a half years hasn't been traveling in America. He's been in some other country!\"\n",
    "print(sentence)\n",
    "print(sentence.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cela nous pouvons utiliser une regex qui va nous permettre de détecter les phrases contenant des ponctuations non désiré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anybody that says America has been standing still for the last seven and a half years hasn't been traveling in America He's been in some other country\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "new_s = re.sub(r'[\\.!\\?]','',sentence)\n",
    "print(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# vider le tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found_3 = []\n",
    "\n",
    "#loop through the components_data that have not been found\n",
    "for index, row in components_data.loc[components_not_found_2].iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text but this time we remove the ponctuation\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.replace(r'[\\.\\!\\?]', '', regex=True).str.find(textToFind) != -1]\n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        component = \"\"\n",
    "        print(index)\n",
    "        first_id = -1\n",
    "        last_id = -1\n",
    "        # for each sentence of the speech\n",
    "        for sentence in sentences:\n",
    "            # if the sentence is in the component text\n",
    "            if textToFind.find(sentence.replace(r'[\\.\\!\\?]','',regex=True)) != -1:\n",
    "                if first_id == -1:\n",
    "                    first_id = sentences.index(sentence)\n",
    "                last_id = sentences.index(sentence)\n",
    "                # concat the sentence to the component\n",
    "                component = component + ' ' + sentence\n",
    "        # update the component text\n",
    "        components_data.loc[index, 'Text'] = component\n",
    "        # update the component text in the components_data_context\n",
    "        components_data_context.loc[index, 'Text'] = component\n",
    "        # add the previous sentence\n",
    "        context1 = ''\n",
    "        if first_id > 0:\n",
    "            context1 += sentences[first_id-1]\n",
    "        context1 += ' ' + component\n",
    "        if last_id < len(sentences)-1:\n",
    "            context1 += ' ' + sentences[last_id+1]\n",
    "        # update the context1\n",
    "        components_data_context.loc[index, 'Context1'] = context1\n",
    "        # update the context2\n",
    "        components_data_context.loc[index, 'Context2'] = speeches['Speech'].values[0]\n",
    "        \n",
    "    except:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found_3.append(index)\n",
    "        # print('Component not found in speeches: ' + str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tax program of the Ford administration will bring jobs where people are, and help to revitalize those cities as they can be\n"
     ]
    }
   ],
   "source": [
    "# print le composant d'ID 603\n",
    "print(components_data.loc[603]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>SectionID</th>\n",
       "      <th>SpeechID</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1976</td>\n",
       "      <td>22Oct</td>\n",
       "      <td>FORD</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>Let me uh - speak out very strongly. The Ford...</td>\n",
       "      <td>3485</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year   Date Speaker SectionID  SpeechID  \\\n",
       "62  1976  22Oct    FORD         8         4   \n",
       "\n",
       "                                               Speech  Start   End  \n",
       "62   Let me uh - speak out very strongly. The Ford...   3485  5213  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search the speech that contains the word 'the tax program'\n",
    "speeches_data[speeches_data['Speech'].str.find('the tax program of the Ford administration') != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "the tax program of the Ford administration will bring jobs where people are, and help to revitalize those cities as they can be\n"
     ]
    }
   ],
   "source": [
    "m_string = 'the tax program of the Ford administration will bring jobs where people'\n",
    "\n",
    "# try all speechs\n",
    "validation_speeches = pd.read_csv('data/validation_speeches.csv')\n",
    "print(len(validation_speeches[validation_speeches['Speech'].str.find(m_string) != -1]))\n",
    "\n",
    "train_speeches = pd.read_csv('data/train_speeches.csv')\n",
    "print(len(train_speeches[train_speeches['Speech'].str.find(m_string) != -1]))\n",
    "\n",
    "test_speeches = pd.read_csv('data/test_speeches.csv')\n",
    "print(len(test_speeches[test_speeches['Speech'].str.find(m_string) != -1]))\n",
    "\n",
    "# this component didn't exist at all ? \n",
    "# print le composant d'ID 603\n",
    "print(components_data.loc[603]['Text'])\n",
    "# Actualy, this component exist but in this case, the sentence, was split in three and we have only the first part and last part of the sentence\n",
    "# the results is that we can't find the component in the speech but it is the speech 62 by Ford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(components_not_found_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                                                              1960\n",
       "Date                                                             21Oct\n",
       "SectionID                                                            9\n",
       "ID                                                                T448\n",
       "SpeechID                                                             3\n",
       "Label                                                            Claim\n",
       "Text                 Anybody that says America has been standing st...\n",
       "Start                                                             5524\n",
       "End                                                               5675\n",
       "SentenceID_begin                                                    11\n",
       "SentenceID_end                                                      12\n",
       "Current_Sentence     Anybody that says America has been standing st...\n",
       "Previous_Sentence    We find four times as many projects undertaken...\n",
       "Next_Sentence                      Let's get that straight right away.\n",
       "Speaker                                                          NIXON\n",
       "Name: 75, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_data.loc[components_not_found_3[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traiter tous les cas particuliers\n",
    "\n",
    "Identifications des cas particuliers:\n",
    "\n",
    "* Certaines phrases sont tronqués, il faut donc les compléter.\n",
    "* Manque de ponctuation (manque des points par exemple)\n",
    "\n",
    "Nombre d'éléments non trouvés: 42 (dans le dataset test)\n",
    "Ponctuation manquante: 2\n",
    "Ponctuation manquante: [75, 95, ]\n",
    "Correspondance:[8, 8, ]\n",
    "Phrase tronquée: 2\n",
    "Phrase tronquée: [603, 927, ]\n",
    "Correspondance:[62, 103, ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do make a pledge that in the next ten days when we're asking the American people to make one of the most important decisions in their lifetime that uh - we do together what we can to stimulate voter participation\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_data.loc[components_not_found_3[3]]['Text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>SectionID</th>\n",
       "      <th>SpeechID</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1976</td>\n",
       "      <td>22Oct</td>\n",
       "      <td>FORD</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>I believe that the uh - American people have ...</td>\n",
       "      <td>3208</td>\n",
       "      <td>4735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year   Date Speaker SectionID  SpeechID  \\\n",
       "103  1976  22Oct    FORD         2         4   \n",
       "\n",
       "                                                Speech  Start   End  \n",
       "103   I believe that the uh - American people have ...   3208  4735  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_data.loc[speeches_data.Speech.str.find(\"I do make a pledge that in the next ten days when we're asking the American people to make one of the most important decisions in \") != -1]\n",
    "#  I believe that the uh - American people have been turned off in this election, uh - Mr. Maynard, for a variety of reasons. We have seen on Capitol Hill, in the Congress, uh - a great many uh - allegations of wrong-doing, of uh - alleged immorality, uh - those are very disturbing to the American people. They wonder how an elected representative uh - can serve them and participate in such activities uh - serving in the Congress of the United States. Yes, and I'm certain many, many Americans were turned off by the revelations of Watergate, a very, very uh - bad period of time in American political history. Yes, and thousands, maybe millions of Americans were turned off because of the uh - problems that came out of our involvement in Vietnam. But on the other hand, I found on July fourth of this year, a new spirit born in America. We were celebrating our Bicentennial; and I find that uh - there is a - a movement as I travel around the country of greater interest in this campaign. Now, like uh - any hardworking uh - person seeking public office uh - in the campaign, inevitably sometimes you will use uh - rather graphic language and I'm guilty of that just like I think most others in the political arena. But I do make a pledge that in the next ten days when we're asking the American people to make one of the most important decisions in their lifetime, because I think this election is one of the mast vital in the history of America, that uh - we do together what we can to stimulate voter participation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayer de faire une recherche automatique des phrases qui sont tronquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      " Let me uh - speak out very strongly. The Ford administration does have a very comprehensive program to help uh - our major metropolitan areas. I fought for, and the Congress finally went along with a general revenue sharing program, whereby cities and uh - states, uh - the cities two-thirds and the states one-third, get over six billion dollars a year in cash through which they can uh - provide many, many services, whatever they really want. In addition we uh - in the federal government make available to uh - cities about uh - three billion three hundred million dollars in what we call community development. In adesh- in addition, uh - uh - as a result of my pressure an the Congress, we got a major mass transit program uh - over a four-year period, eleven billion eight-hundred million dollars. We have a good housing program, uh - that uh - will result in cutting uh - the down payments by 50 percent and uh - having mortgage payments uh lower at the beginning of any mortgage period. We're expanding our homestead uh - housing program. The net result is uh - we think under Carla Hills, who's the chairman of my uh - urban development and uh - neighborhood revitalization program, we will really do a first-class job in helping uh - the communities throughout the country. As a matter of fact, that committee under Secretary Hills released about a seventy-five-page report with specific recommendations so we can do a better job uh - the weeks ahead. And in addition, the tax program of the Ford administration, which provides an incentive for industry to move into our major uh - metropolitan areas, into the inner cities, will bring jobs where people are, and help to revitalize those cities as they can be.\n",
      "\n",
      " Let me uh - speak out very strongly. The Ford administration does have a very comprehensive program to help uh - our major metropolitan areas. I fought for, and the Congress finally went along with a general revenue sharing program, whereby cities and uh - states, uh - the cities two-thirds and the states one-third, get over six billion dollars a year in cash through which they can uh - provide many, many services, whatever they really want. In addition we uh - in the federal government make available to uh - cities about uh - three billion three hundred million dollars in what we call community development. In adesh- in addition, uh - uh - as a result of my pressure an the Congress, we got a major mass transit program uh - over a four-year period, eleven billion eight-hundred million dollars. We have a good housing program, uh - that uh - will result in cutting uh - the down payments by 50 percent and uh - having mortgage payments uh lower at the beginning of any mortgage period. We're expanding our homestead uh - housing program. The net result is uh - we think under Carla Hills, who's the chairman of my uh - urban development and uh - neighborhood revitalization program, we will really do a first-class job in helping uh - the communities throughout the country. As a matter of fact, that committee under Secretary Hills released about a seventy-five-page report with specific recommendations so we can do a better job uh - the weeks ahead. And in addition, the tax program of the Ford administration, which provides an incentive for industry to move into our major uh - metropolitan areas, into the inner cities, will bring jobs where people are, and help to revitalize those cities as they can be.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get le component d'ID 603\n",
    "comp = components_data.loc[603]\n",
    "# recuperer le texte du component\n",
    "text = comp['Text']\n",
    "# split le text sur les espaces\n",
    "tab_text = text.split(' ')\n",
    "\n",
    "# tester jusqu'où la phrase est présente dans le texte (non tronqué)\n",
    "startComp = tab_text[0]\n",
    "while(len(speeches_data.loc[speeches_data.Speech.str.find(startComp) != -1]) > 0):\n",
    "    # verifier que la phrase ne soit présente que dans un seul speech (meilleur des cas)\n",
    "    tab_text.pop(0)\n",
    "    startComp += ' '+tab_text[0]\n",
    "\n",
    "# enelever le dernier mot de current\n",
    "startComp = ' '.join(startComp.split(' ')[:-1])\n",
    "# print(speeches_data.loc[speeches_data.Speech.str.find(startComp) != -1])\n",
    "speechs = speeches_data.loc[speeches_data.Speech.str.find(startComp) != -1]\n",
    "endCom = ' '.join(tab_text)\n",
    "m_speech = []\n",
    "for index, speech in zip(speechs.index, speechs.Speech):\n",
    "    if(speech.find(endCom) != -1):\n",
    "        print(index)\n",
    "        print(speech)\n",
    "        m_speech = speech\n",
    "# une fois que le debut du component est trouvé, verifier que la fin du component est aussi dans le speech (et ainsi retrouvé les deux partie de la phrase)\n",
    "print(m_speech)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
