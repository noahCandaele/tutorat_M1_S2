{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorat M1 S2 Informatique DS4H\n",
    "\n",
    "### Subject:\n",
    "\n",
    "#### Re-structuration intelligente d'un jeu de donnée de debats politiques pour l'extraction de structures argumentaires\n",
    "\n",
    "L'objectif du travail à réaliser est de structurer des données nécessaires à l'étude des composantes et des relations au sein des débats politiques qui ont eu lieu lors des élections du président des États-Unis de 1960 à 2016. Les débats se présentent sous la forme d'un dialogue entre un candidat et l'autre, qui répondent aux questions posées par un orateur sur divers sujets tels que l'économie, la sécurité, l'éducation, la guerre, les soins de santé, etc. Chaque débat a été divisé en sections en tenant compte des différents sujets abordés.\n",
    "\n",
    "Tous les débats ont été annotés d'un point de vue argumentatif. Des annotations concernant les composantes argumentatives sont présentes à l'intérieur :\n",
    "- Conclusions\n",
    "- Prémisses\n",
    "et des annotations faisant référence aux relations entre ces deux composants :\n",
    "- Attaque\n",
    "- Soutien\n",
    "- Équivalent\n",
    "\n",
    "L'objectif du projet est de concevoir et implémenter une structure de données textuels qui soit facile à manipuler pour la réalisation d'une des nombreuses tâches du TAL, à savoir l'extraction d'arguments.\n",
    "Plus précisément, il s’agit de deux structures :\n",
    "1) Un jeu de données référençant les composants (Claim, Premise) représentés par les colonnes suivantes :\n",
    "- Ligne de dialogue\n",
    "- Composants de l'argumentation\n",
    "- Schéma BIO des composants\n",
    "- Caractéristiques linguistiques, lexicales, grammaticales, syntaxiques, etc... (Chaque caractéristique séparée par une colonne) concernant le composant considéré\n",
    "2) Un ensemble de données se référant aux relations (Attaque, Soutien, Équivalent) regroupées par section et représentées par les colonnes suivantes :\n",
    "- Composante 1 (Claim/Premise)\n",
    "- Composante 2 (Claim/Premise)\n",
    "- Type de relation (Attaque/Soutien/Équivalent)\n",
    "- Schéma BIO des composants et des relations avec leur distance\n",
    "- Caractéristiques linguistiques, lexicales, grammaticales, syntaxiques, etc. (chaque caractéristique séparée par une colonne) concernant la relation considérée"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "* What if the component is two sentences ? Because we don't have any ponctuation in the current component dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data file\n",
    "components_data = pd.read_csv('./data/test_components.csv')\n",
    "speeches_data = pd.read_csv('./data/test_speeches.csv')\n",
    "# work with only the first 5 rows\n",
    "# components_data = components_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe that copy the components_data but without the \"Previous_Sentence\" and \"Next_Sentence\" columns\n",
    "components_data_context = components_data.drop(['Current_Sentence', 'Previous_Sentence', 'Next_Sentence'], axis=1)\n",
    "# add columns for contexts\n",
    "components_data_context['Context1'] = ''\n",
    "components_data_context['Context2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component not found in speeches: 56\n",
      "Component not found in speeches: 75\n",
      "Component not found in speeches: 94\n",
      "Component not found in speeches: 95\n",
      "Component not found in speeches: 603\n",
      "Component not found in speeches: 927\n",
      "Component not found in speeches: 935\n",
      "Component not found in speeches: 1335\n",
      "Component not found in speeches: 1410\n",
      "Component not found in speeches: 1416\n",
      "Component not found in speeches: 1429\n",
      "Component not found in speeches: 1525\n",
      "Component not found in speeches: 1709\n",
      "Component not found in speeches: 3552\n",
      "Component not found in speeches: 4821\n",
      "Component not found in speeches: 5416\n",
      "Component not found in speeches: 5434\n",
      "Component not found in speeches: 5521\n",
      "Component not found in speeches: 5537\n",
      "Component not found in speeches: 5579\n",
      "Component not found in speeches: 5583\n",
      "Component not found in speeches: 5601\n",
      "Component not found in speeches: 5644\n",
      "Component not found in speeches: 5650\n",
      "Component not found in speeches: 5757\n",
      "Component not found in speeches: 5817\n",
      "Component not found in speeches: 6118\n",
      "Component not found in speeches: 6124\n",
      "Component not found in speeches: 6188\n",
      "Component not found in speeches: 6208\n",
      "Component not found in speeches: 6806\n",
      "Component not found in speeches: 7325\n",
      "Component not found in speeches: 7582\n",
      "Component not found in speeches: 7590\n",
      "Component not found in speeches: 8316\n",
      "Component not found in speeches: 8573\n",
      "Component not found in speeches: 8827\n",
      "Component not found in speeches: 8850\n",
      "Component not found in speeches: 8932\n",
      "Component not found in speeches: 8963\n",
      "Component not found in speeches: 8981\n",
      "Component not found in speeches: 9026\n",
      "Component not found in speeches: 9027\n",
      "Component not found in speeches: 9029\n"
     ]
    }
   ],
   "source": [
    "# tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found = []\n",
    "\n",
    "# for each component, find the speeches that mention it\n",
    "for index, row in components_data.iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.find(textToFind) != -1]\n",
    "    \n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        # tokenize the speech into sentences\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        \n",
    "        context1 = ''\n",
    "        # get the sentence that contains the component\n",
    "        for index, sentence in zip(range(0,len(sentences)), sentences):\n",
    "            if sentence.find(textToFind) != -1:\n",
    "                if(index > 0):\n",
    "                    context1 = sentences[index-1]\n",
    "                # add the current sentence\n",
    "                context1 = context1 + ' ' + sentence\n",
    "                if(index < len(sentences)-1):\n",
    "                    context1 = context1 + ' ' + sentences[index+1]\n",
    "                break\n",
    "        # add context1\n",
    "        components_data_context.loc[index, 'Context1'] = context1\n",
    "        # add full speech to the context2 column\n",
    "        components_data_context.loc[index, 'Context2'] = speeches['Speech'].values[0]\n",
    "    except:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found.append(index)\n",
    "        print('Component not found in speeches: ' + str(index))\n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un certain nombre de composants n'ont pas été trouvés dans le texte, cela peut être du au format dans lesquels les données ont été enregistrées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of components not found: 0.4634994206257242%\n"
     ]
    }
   ],
   "source": [
    "# Un certain nombre de components n'ont pas été trouvés dans les speeches\n",
    "print('Percentage of components not found: ' + str(len(components_not_found)/len(components_data)*100) + '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suite va donc consister à détecter les différents formats pour pouvoir les patcher et les ajouter à notre premier contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still We can no longer afford to be second best\n"
     ]
    }
   ],
   "source": [
    "# prenons pour exemple le premier component qui n'a pas été trouvé\n",
    "cps = components_data.loc[components_not_found[0]]\n",
    "print(cps.Text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici le composent (id 56) est composé de 2 phrases, or la fonction sent_tokenize ne les détecte pas comme 2 phrases car il a été stocké sans la ponctuation.\n",
    "\n",
    "On va donc refaire une boucle mais cette fois sans la ponctuation pour détecter les phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence found: I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still.\n",
      "Component text: I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still We can no longer afford to be second best\n",
      "Sentence found: We can no longer afford to be second best.\n",
      "Component text: I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still We can no longer afford to be second best\n",
      "Sentence found: There were eleven dictators when we came into power in 1953 in Latin America.\n",
      "Component text: There were eleven dictators when we came into power in 1953 in Latin America There are only three left\n",
      "Sentence found: There are only three left.\n",
      "Component text: There were eleven dictators when we came into power in 1953 in Latin America There are only three left\n"
     ]
    }
   ],
   "source": [
    "# vider le tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found_2 = []\n",
    "\n",
    "#loop through the components_data that have not been found\n",
    "for index, row in components_data.loc[components_not_found].iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text but this time we remove the ponctuation\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.translate(str.maketrans('', '', string.punctuation)).str.find(textToFind) != -1]\n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        # for each sentence of the speech\n",
    "        for sentence in sentences:\n",
    "            # if the sentence is in the component text\n",
    "            if textToFind.find(sentence.translate(str.maketrans('', '', string.punctuation))) != -1:\n",
    "                print(\"Sentence found: \" + sentence)\n",
    "                print(\"Component text: \" + textToFind)\n",
    "    except:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found_2.append(index)\n",
    "        # print('Component not found in speeches: ' + str(index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
