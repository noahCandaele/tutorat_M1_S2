{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorat M1 S2 Informatique DS4H\n",
    "\n",
    "### Subject:\n",
    "\n",
    "#### Re-structuration intelligente d'un jeu de donnée de debats politiques pour l'extraction de structures argumentaires\n",
    "\n",
    "L'objectif du travail à réaliser est de structurer des données nécessaires à l'étude des composantes et des relations au sein des débats politiques qui ont eu lieu lors des élections du président des États-Unis de 1960 à 2016. Les débats se présentent sous la forme d'un dialogue entre un candidat et l'autre, qui répondent aux questions posées par un orateur sur divers sujets tels que l'économie, la sécurité, l'éducation, la guerre, les soins de santé, etc. Chaque débat a été divisé en sections en tenant compte des différents sujets abordés.\n",
    "\n",
    "Tous les débats ont été annotés d'un point de vue argumentatif. Des annotations concernant les composantes argumentatives sont présentes à l'intérieur :\n",
    "- Conclusions\n",
    "- Prémisses\n",
    "et des annotations faisant référence aux relations entre ces deux composants :\n",
    "- Attaque\n",
    "- Soutien\n",
    "- Équivalent\n",
    "\n",
    "L'objectif du projet est de concevoir et implémenter une structure de données textuels qui soit facile à manipuler pour la réalisation d'une des nombreuses tâches du TAL, à savoir l'extraction d'arguments.\n",
    "Plus précisément, il s’agit de deux structures :\n",
    "1) Un jeu de données référençant les composants (Claim, Premise) représentés par les colonnes suivantes :\n",
    "- Ligne de dialogue\n",
    "- Composants de l'argumentation\n",
    "- Schéma BIO des composants\n",
    "- Caractéristiques linguistiques, lexicales, grammaticales, syntaxiques, etc... (Chaque caractéristique séparée par une colonne) concernant le composant considéré\n",
    "2) Un ensemble de données se référant aux relations (Attaque, Soutien, Équivalent) regroupées par section et représentées par les colonnes suivantes :\n",
    "- Composante 1 (Claim/Premise)\n",
    "- Composante 2 (Claim/Premise)\n",
    "- Type de relation (Attaque/Soutien/Équivalent)\n",
    "- Schéma BIO des composants et des relations avec leur distance\n",
    "- Caractéristiques linguistiques, lexicales, grammaticales, syntaxiques, etc. (chaque caractéristique séparée par une colonne) concernant la relation considérée"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "* What if the component is two sentences ? Because we don't have any ponctuation in the current component dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data file\n",
    "components_data = pd.read_csv('./data/test_components.csv')\n",
    "speeches_data = pd.read_csv('./data/test_speeches.csv')\n",
    "# work with only the first 5 rows\n",
    "# components_data = components_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe that copy the components_data but without the \"Previous_Sentence\" and \"Next_Sentence\" columns\n",
    "components_data_context = components_data.drop(['Current_Sentence', 'Previous_Sentence', 'Next_Sentence'], axis=1)\n",
    "# add columns for contexts\n",
    "components_data_context['Context1'] = ''\n",
    "components_data_context['Context2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component not found in speeches: 56\n",
      "Component not found in speeches: 75\n",
      "Component not found in speeches: 94\n",
      "Component not found in speeches: 95\n",
      "Component not found in speeches: 603\n",
      "Component not found in speeches: 927\n",
      "Component not found in speeches: 935\n",
      "Component not found in speeches: 1335\n",
      "Component not found in speeches: 1410\n",
      "Component not found in speeches: 1416\n",
      "Component not found in speeches: 1429\n",
      "Component not found in speeches: 1525\n",
      "Component not found in speeches: 1709\n",
      "Component not found in speeches: 3552\n",
      "Component not found in speeches: 4821\n",
      "Component not found in speeches: 5416\n",
      "Component not found in speeches: 5434\n",
      "Component not found in speeches: 5521\n",
      "Component not found in speeches: 5537\n",
      "Component not found in speeches: 5579\n",
      "Component not found in speeches: 5583\n",
      "Component not found in speeches: 5601\n",
      "Component not found in speeches: 5644\n",
      "Component not found in speeches: 5650\n",
      "Component not found in speeches: 5757\n",
      "Component not found in speeches: 5817\n",
      "Component not found in speeches: 6118\n",
      "Component not found in speeches: 6124\n",
      "Component not found in speeches: 6188\n",
      "Component not found in speeches: 6208\n",
      "Component not found in speeches: 6806\n",
      "Component not found in speeches: 7325\n",
      "Component not found in speeches: 7582\n",
      "Component not found in speeches: 7590\n",
      "Component not found in speeches: 8316\n",
      "Component not found in speeches: 8573\n",
      "Component not found in speeches: 8827\n",
      "Component not found in speeches: 8850\n",
      "Component not found in speeches: 8932\n",
      "Component not found in speeches: 8963\n",
      "Component not found in speeches: 8981\n",
      "Component not found in speeches: 9026\n",
      "Component not found in speeches: 9027\n",
      "Component not found in speeches: 9029\n"
     ]
    }
   ],
   "source": [
    "# tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found = []\n",
    "\n",
    "# for each component, find the speeches that mention it\n",
    "for index, row in components_data.iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.find(textToFind) != -1]\n",
    "    \n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        # tokenize the speech into sentences\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        \n",
    "        context1 = ''\n",
    "        # get the sentence that contains the component\n",
    "        for index, sentence in zip(range(0,len(sentences)), sentences):\n",
    "            if sentence.find(textToFind) != -1:\n",
    "                if(index > 0):\n",
    "                    context1 = sentences[index-1]\n",
    "                # add the current sentence\n",
    "                context1 = context1 + ' ' + sentence\n",
    "                if(index < len(sentences)-1):\n",
    "                    context1 = context1 + ' ' + sentences[index+1]\n",
    "                break\n",
    "        # add context1\n",
    "        components_data_context.loc[index, 'Context1'] = context1\n",
    "        # add full speech to the context2 column\n",
    "        components_data_context.loc[index, 'Context2'] = speeches['Speech'].values[0]\n",
    "    except:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found.append(index)\n",
    "        print('Component not found in speeches: ' + str(index))\n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un certain nombre de composants n'ont pas été trouvés dans le texte, cela peut être du au format dans lesquels les données ont été enregistrées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of components not found: 0.4634994206257242%\n"
     ]
    }
   ],
   "source": [
    "# Un certain nombre de components n'ont pas été trouvés dans les speeches\n",
    "print('Percentage of components not found: ' + str(len(components_not_found)/len(components_data)*100) + '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suite va donc consister à détecter les différents formats pour pouvoir les patcher et les ajouter à notre premier contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe it my responsibility as the leader of the Democratic party in 1960 to try to warn the American people that in this crucial time we can no longer afford to stand still We can no longer afford to be second best\n"
     ]
    }
   ],
   "source": [
    "# prenons pour exemple le premier component qui n'a pas été trouvé\n",
    "cps = components_data.loc[components_not_found[0]]\n",
    "print(cps.Text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici le composent (id 56) est composé de 2 phrases, or la fonction sent_tokenize ne les détecte pas comme 2 phrases car il a été stocké sans la ponctuation.\n",
    "\n",
    "On va donc refaire une boucle mais cette fois sans la ponctuation pour détecter les phrases.\n",
    "(petit soucis avec cette technique, les phrases contenant les ' ne sont pas détectées, on va donc pas la suite essayer de trouver une autre solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n"
     ]
    }
   ],
   "source": [
    "# vider le tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found_2 = []\n",
    "\n",
    "#loop through the components_data that have not been found\n",
    "for index, row in components_data.loc[components_not_found].iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text but this time we remove the ponctuation\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.translate(str.maketrans('', '', string.punctuation)).str.find(textToFind) != -1]\n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        \n",
    "        component = \"\"\n",
    "        first_id = -1\n",
    "        last_id = -1\n",
    "        # for each sentence of the speech\n",
    "        for sentence in sentences:\n",
    "            # if the sentence is in the component text\n",
    "            if textToFind.find(sentence.translate(str.maketrans('', '', string.punctuation))) != -1:\n",
    "                if first_id == -1:\n",
    "                    first_id = sentences.index(sentence)\n",
    "                last_id = sentences.index(sentence)\n",
    "                # concat the sentence to the component\n",
    "                component = component + ' ' + sentence\n",
    "        # update the component text\n",
    "        components_data.loc[index, 'Text'] = component\n",
    "        # update the component text in the components_data_context\n",
    "        components_data_context.loc[index, 'Text'] = component\n",
    "        # add the previous sentence\n",
    "        context1 = ''\n",
    "        if first_id > 0:\n",
    "            context1 += sentences[first_id-1]\n",
    "        context1 += ' ' + component\n",
    "        if last_id < len(sentences)-1:\n",
    "            context1 += ' ' + sentences[last_id+1]\n",
    "        # update the context1\n",
    "        components_data_context.loc[index, 'Context1'] = context1\n",
    "        # update the context2\n",
    "        components_data_context.loc[index, 'Context2'] = speeches['Speech'].values[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found_2.append(index)\n",
    "        # print('Component not found in speeches: ' + str(index))\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechID                                                     3\n",
      "SectionID                                                    9\n",
      "Text         Anybody that says America has been standing st...\n",
      "Name: 75, dtype: object\n",
      "Percentage of components not found: 0.44243126514273673%\n"
     ]
    }
   ],
   "source": [
    "# get first component that has not been found\n",
    "comp = components_data.loc[components_not_found_2[0]]\n",
    "# print the speechid and the section id and the text of the component\n",
    "print(comp[['SpeechID', 'SectionID', 'Text']])\n",
    "# print the percentage of components that have not been found\n",
    "print('Percentage of components not found: ' + str(len(components_not_found_2)/len(components_data)*100) + '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, le pourcentage n'a pas beaucoup changé, on va donc essayer de trouver une autre solution.\n",
    "Le principal soucis ici est que les phrases contenant des ' ne sont pas détectées. Ce qui est un cas fréquent en anglais. Il faut donc que l'on trouve une solution pour détecter ces phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anybody that says America has been standing still for the last seven and a half years hasn't been traveling in America. He's been in some other country!\n",
      "Anybody that says America has been standing still for the last seven and a half years hasnt been traveling in America Hes been in some other country\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Anybody that says America has been standing still for the last seven and a half years hasn't been traveling in America. He's been in some other country!\"\n",
    "print(sentence)\n",
    "print(sentence.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cela nous pouvons utiliser une regex qui va nous permettre de détecter les phrases contenant des ponctuations non désiré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anybody that says America has been standing still for the last seven and a half years hasn't been traveling in America He's been in some other country\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "new_s = re.sub(r'[\\.!\\?]','',sentence)\n",
    "print(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# vider le tableau d'index des components qui n'ont pas été trouvés\n",
    "components_not_found_3 = []\n",
    "\n",
    "#loop through the components_data that have not been found\n",
    "for index, row in components_data.loc[components_not_found_2].iterrows():\n",
    "    textToFind = row.Text\n",
    "    # find the speeches that have the component in the text but this time we remove the ponctuation\n",
    "    speeches = speeches_data[speeches_data['Speech'].str.replace(r'[\\.\\!\\?]', '', regex=True).str.find(textToFind) != -1]\n",
    "    # make a try catch block to handle the case where the component is not found in the speeches\n",
    "    try:\n",
    "        sentences = sent_tokenize(speeches['Speech'].values[0])\n",
    "        component = \"\"\n",
    "        print(index)\n",
    "        first_id = -1\n",
    "        last_id = -1\n",
    "        # for each sentence of the speech\n",
    "        for sentence in sentences:\n",
    "            # if the sentence is in the component text\n",
    "            if textToFind.find(sentence.replace(r'[\\.\\!\\?]','',regex=True)) != -1:\n",
    "                if first_id == -1:\n",
    "                    first_id = sentences.index(sentence)\n",
    "                last_id = sentences.index(sentence)\n",
    "                # concat the sentence to the component\n",
    "                component = component + ' ' + sentence\n",
    "        # update the component text\n",
    "        components_data.loc[index, 'Text'] = component\n",
    "        # update the component text in the components_data_context\n",
    "        components_data_context.loc[index, 'Text'] = component\n",
    "        # add the previous sentence\n",
    "        context1 = ''\n",
    "        if first_id > 0:\n",
    "            context1 += sentences[first_id-1]\n",
    "        context1 += ' ' + component\n",
    "        if last_id < len(sentences)-1:\n",
    "            context1 += ' ' + sentences[last_id+1]\n",
    "        # update the context1\n",
    "        components_data_context.loc[index, 'Context1'] = context1\n",
    "        # update the context2\n",
    "        components_data_context.loc[index, 'Context2'] = speeches['Speech'].values[0]\n",
    "        \n",
    "    except:\n",
    "        # stocker les index des components qui n'ont pas été trouvés\n",
    "        components_not_found_3.append(index)\n",
    "        # print('Component not found in speeches: ' + str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tax program of the Ford administration will bring jobs where people are, and help to revitalize those cities as they can be\n"
     ]
    }
   ],
   "source": [
    "# print le composant d'ID 603\n",
    "print(components_data.loc[603]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>SectionID</th>\n",
       "      <th>SpeechID</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1976</td>\n",
       "      <td>22Oct</td>\n",
       "      <td>FORD</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>Let me uh - speak out very strongly. The Ford...</td>\n",
       "      <td>3485</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year   Date Speaker SectionID  SpeechID  \\\n",
       "62  1976  22Oct    FORD         8         4   \n",
       "\n",
       "                                               Speech  Start   End  \n",
       "62   Let me uh - speak out very strongly. The Ford...   3485  5213  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search the speech that contains the word 'the tax program'\n",
    "speeches_data[speeches_data['Speech'].str.find('the tax program of the Ford administration') != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "the tax program of the Ford administration will bring jobs where people are, and help to revitalize those cities as they can be\n"
     ]
    }
   ],
   "source": [
    "m_string = 'the tax program of the Ford administration will bring jobs where people'\n",
    "\n",
    "# try all speechs\n",
    "validation_speeches = pd.read_csv('data/validation_speeches.csv')\n",
    "print(len(validation_speeches[validation_speeches['Speech'].str.find(m_string) != -1]))\n",
    "\n",
    "train_speeches = pd.read_csv('data/train_speeches.csv')\n",
    "print(len(train_speeches[train_speeches['Speech'].str.find(m_string) != -1]))\n",
    "\n",
    "test_speeches = pd.read_csv('data/test_speeches.csv')\n",
    "print(len(test_speeches[test_speeches['Speech'].str.find(m_string) != -1]))\n",
    "\n",
    "# this component didn't exist at all ? \n",
    "# print le composant d'ID 75\n",
    "print(components_data.loc[603]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                                                              1976\n",
      "Date                                                             22Oct\n",
      "SectionID                                                            2\n",
      "ID                                                                 T86\n",
      "SpeechID                                                             0\n",
      "Label                                                            Claim\n",
      "Text                 I do make a pledge that in the next ten days w...\n",
      "Start                                                               -1\n",
      "End                                                                213\n",
      "SentenceID_begin                                                     0\n",
      "SentenceID_end                                                       0\n",
      "Current_Sentence                                   WALTERS: Thank you.\n",
      "Previous_Sentence                                  WALTERS: Thank you.\n",
      "Next_Sentence          Mr. Maynard, your question for Governor Carter.\n",
      "Speaker                                                        WALTERS\n",
      "Name: 927, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print le component d'id 927\n",
    "print(components_data.loc[927])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
